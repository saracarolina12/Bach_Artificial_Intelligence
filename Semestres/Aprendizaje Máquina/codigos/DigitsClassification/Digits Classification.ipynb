{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76923cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datasets\n",
    "from sklearn.datasets import load_iris, load_digits, load_breast_cancer\n",
    "\n",
    "# Training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3b4004",
   "metadata": {},
   "source": [
    "## First Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e4c60df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target names [0 1 2 3 4 5 6 7 8 9]\n",
      "data shape (1797, 64)\n",
      "X (1797, 64)\n",
      "Y 1797 (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([178, 182, 177, 183, 181, 182, 181, 179, 174, 180], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "data = load_digits()\n",
    "X,Y = load_digits(return_X_y=True)\n",
    "print('Target names',data.target_names)\n",
    "print('data shape',data.data.shape)\n",
    "print('X',X.shape)\n",
    "print('Y',len(Y),np.unique(Y,return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ee930e",
   "metadata": {},
   "source": [
    "### Randomly divide the data into training (70%) and testing (30%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1be6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1797, 64) Y: 1797\n",
      "Xtrain: (1257, 64) Ytrain: 1257\n",
      "Xtest: (540, 64) Ytest: 540\n",
      "Ytest: [2 8 2 6 6 7 1 9 8 5 2 8 6 6 6 6 1 0 5 8 8 7 8 4 7 5 4 9 2 9 4 7 6 8 9 4 3\n",
      " 1 0 1 8 6 7 7 1 0 7 6 2 1 9 6 7 9 0 0 5 1 6 3 0 2 3 4 1 9 2 6 9 1 8 3 5 1\n",
      " 2 8 2 2 9 7 2 3 6 0 5 3 7 5 1 2 9 9 3 1 7 7 4 8 5 8 5 5 2 5 9 0 7 1 4 7 3\n",
      " 4 8 9 7 9 8 2 6 5 2 5 8 4 8 7 0 6 1 5 9 9 9 5 9 9 5 7 5 6 2 8 6 9 6 1 5 1\n",
      " 5 9 9 1 5 3 6 1 8 9 8 7 6 7 6 5 6 0 8 8 9 8 6 1 0 4 1 6 3 8 6 7 4 5 6 3 0\n",
      " 3 3 3 0 7 7 5 7 8 0 7 8 9 6 4 5 0 1 4 6 4 3 3 0 9 5 9 2 1 4 2 1 6 8 9 2 4\n",
      " 9 3 7 6 2 3 3 1 6 9 3 6 3 2 2 0 7 6 1 1 9 7 2 7 8 5 5 7 5 2 3 7 2 7 5 5 7\n",
      " 0 9 1 6 5 9 7 4 3 8 0 3 6 4 6 3 2 6 8 8 8 4 6 7 5 2 4 5 3 2 4 6 9 4 5 4 3\n",
      " 4 6 2 9 0 1 7 2 0 9 6 0 4 2 0 7 9 8 5 4 8 2 8 4 3 7 2 6 9 1 5 1 0 8 2 1 9\n",
      " 5 6 8 2 7 2 1 5 1 6 4 5 0 9 4 1 1 7 0 8 9 0 5 4 3 8 8 6 5 3 4 4 4 8 8 7 0\n",
      " 9 6 3 5 2 3 0 8 3 3 1 3 3 0 0 4 6 0 7 7 6 2 0 4 4 2 3 7 8 9 8 6 8 5 6 2 2\n",
      " 3 1 7 7 8 0 3 3 2 1 5 5 9 1 3 7 0 0 7 0 4 5 9 3 3 4 3 1 8 9 8 3 6 2 1 6 2\n",
      " 1 7 5 5 1 9 2 8 9 7 2 1 4 9 3 2 6 2 5 9 6 5 8 2 0 7 8 0 5 8 4 1 8 6 4 3 4\n",
      " 2 0 4 5 8 3 9 1 8 3 4 5 0 8 5 6 3 0 6 9 1 5 2 2 1 9 8 4 3 3 0 7 8 8 1 1 3\n",
      " 5 5 8 4 9 7 8 4 4 9 0 1 6 9 3 6 1 7 0 6 2 9]\n"
     ]
    }
   ],
   "source": [
    "X,Y = load_digits(return_X_y=True)\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.30, random_state=0) #Test 30%, training 70%\n",
    "\n",
    "print('X:',X.shape,'Y:',len(Y) )\n",
    "print('Xtrain:',Xtrain.shape,'Ytrain:',len(Ytrain) )\n",
    "print('Xtest:',Xtest.shape,'Ytest:',len(Ytest) )\n",
    "print('Ytest:',Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61912356",
   "metadata": {},
   "source": [
    "### Generate three classification models (Naive Bayes, Logistic Regression, Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250dea59",
   "metadata": {},
   "source": [
    "#### - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52b081fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Accuracy: 0.847457627118644\n",
      "*Macro-f1: 0.8641666666666665\n",
      "*Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>85.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>71.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2     3     4      5      6      7     8      9\n",
       "0  100.0   0.0   0.0   0.0   0.0    0.0    0.0    0.0   0.0    0.0\n",
       "1    0.0  85.7   0.0   0.0   0.0    0.0    0.0    0.0  14.3    0.0\n",
       "2    0.0   0.0  50.0   0.0   0.0    0.0    0.0    0.0  50.0    0.0\n",
       "3    0.0   0.0  14.3  71.4   0.0    0.0    0.0    0.0  14.3    0.0\n",
       "4    0.0  11.1   0.0   0.0  77.8    0.0    0.0    0.0  11.1    0.0\n",
       "5    0.0   0.0   0.0   0.0   0.0  100.0    0.0    0.0   0.0    0.0\n",
       "6    0.0   0.0   0.0   0.0   0.0    0.0  100.0    0.0   0.0    0.0\n",
       "7    0.0   0.0   0.0   0.0   0.0    0.0    0.0  100.0   0.0    0.0\n",
       "8    0.0  20.0   0.0   0.0   0.0    0.0    0.0    0.0  80.0    0.0\n",
       "9    0.0   0.0   0.0   0.0   0.0    0.0    0.0    0.0   0.0  100.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(Xtrain,Ytrain)\n",
    "Ypred = model.predict(Xtest)\n",
    "\n",
    "# Accuracy\n",
    "print('*Accuracy:', accuracy_score(Ytest,Ypred))\n",
    "# Macro-f1\n",
    "print('*Macro-f1:', f1_score(Ytest,Ypred,average='macro'))\n",
    "# Confusion Matrix\n",
    "print('*Confusion Matrix:')\n",
    "m = confusion_matrix(Ytest,Ypred)\n",
    "m = m.transpose()\n",
    "m = np.round( (m/np.sum(m,axis=0))*100, 1).transpose() #para que salga en porcentaje\n",
    "df = pd.DataFrame(m,index=data.target_names,columns=data.target_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00101f2c",
   "metadata": {},
   "source": [
    "#### - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27cd955f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Accuracy: 0.9537037037037037\n",
      "*Macro-f1: 0.955208755711063\n",
      "*Confusion Matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>94.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>92.5</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.9</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>96.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2     3     4     5     6     7     8     9\n",
       "0  100.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
       "1    0.0  94.2   0.0   0.0   0.0   0.0   0.0   0.0   3.8   1.9\n",
       "2    0.0   3.8  92.5   3.8   0.0   0.0   0.0   0.0   0.0   0.0\n",
       "3    0.0   0.0   0.0  96.3   0.0   0.0   0.0   0.0   1.9   1.9\n",
       "4    0.0   0.0   0.0   0.0  97.9   0.0   0.0   2.1   0.0   0.0\n",
       "5    0.0   0.0   0.0   0.0   0.0  96.5   0.0   0.0   0.0   3.5\n",
       "6    0.0   1.7   0.0   0.0   0.0   0.0  98.3   0.0   0.0   0.0\n",
       "7    0.0   0.0   0.0   1.9   1.9   0.0   0.0  96.2   0.0   0.0\n",
       "8    0.0   4.9   1.6   0.0   0.0   0.0   0.0   0.0  86.9   6.6\n",
       "9    0.0   0.0   0.0   0.0   0.0   1.8   0.0   0.0   1.8  96.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0)\n",
    "model.fit(Xtrain,Ytrain)\n",
    "Ypred = model.predict(Xtest)\n",
    "\n",
    "# Accuracy\n",
    "print('*Accuracy:', accuracy_score(Ytest,Ypred))\n",
    "# Macro-f1\n",
    "print('*Macro-f1:', f1_score(Ytest,Ypred,average='macro'))\n",
    "# Confusion Matrix\n",
    "print('*Confusion Matrix:')\n",
    "m = confusion_matrix(Ytest,Ypred)\n",
    "m = m.transpose()\n",
    "m = np.round( (m/np.sum(m,axis=0))*100,1).transpose() #para que salga en porcentaje\n",
    "df = pd.DataFrame(m,index=data.target_names,columns=data.target_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7016da2e",
   "metadata": {},
   "source": [
    "#### - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40926d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Accuracy: 0.8574074074074074\n",
      "*Macro-f1: 0.8591807712816509\n",
      "*Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>86.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.1</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>83.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>89.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.3</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>70.5</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3.5</td>\n",
       "      <td>84.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9\n",
       "0  93.3   0.0   0.0   0.0   0.0   0.0   2.2   0.0   0.0   4.4\n",
       "1   0.0  86.5   0.0   0.0   3.8   0.0   3.8   1.9   1.9   1.9\n",
       "2   1.9   0.0  81.1   5.7   1.9   0.0   0.0   0.0   3.8   5.7\n",
       "3   0.0   0.0   3.7  83.3   0.0   1.9   3.7   0.0   7.4   0.0\n",
       "4   2.1   0.0   0.0   0.0  89.6   0.0   4.2   0.0   4.2   0.0\n",
       "5   0.0   0.0   1.8   1.8   1.8  89.5   0.0   0.0   0.0   5.3\n",
       "6   0.0   3.3   1.7   0.0   5.0   0.0  90.0   0.0   0.0   0.0\n",
       "7   1.9   1.9   0.0   0.0   0.0   0.0   0.0  92.5   1.9   1.9\n",
       "8   4.9   4.9   3.3   6.6   0.0   0.0   0.0   3.3  70.5   6.6\n",
       "9   0.0   0.0   1.8   5.3   0.0   3.5   0.0   1.8   3.5  84.2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=0)\n",
    "model.fit(Xtrain,Ytrain)\n",
    "Ypred = model.predict(Xtest)\n",
    "\n",
    "# Accuracy\n",
    "print('*Accuracy:', accuracy_score(Ytest,Ypred))\n",
    "# Macro-f1\n",
    "print('*Macro-f1:', f1_score(Ytest,Ypred,average='macro'))\n",
    "# Confusion Matrix\n",
    "print('*Confusion Matrix:')\n",
    "m = confusion_matrix(Ytest,Ypred)\n",
    "m = m.transpose()\n",
    "m = np.round( (m/np.sum(m,axis=0))*100,1).transpose() #para que salga en porcentaje\n",
    "df = pd.DataFrame(m,index=data.target_names,columns=data.target_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0be71f",
   "metadata": {},
   "source": [
    "## Second Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd93616",
   "metadata": {},
   "source": [
    "### Randomly divide the data into 30 splits using K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b31682fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X,Y = load_digits(return_X_y=True)\n",
    "kf = KFold(n_splits=30, random_state=0, shuffle=True)\n",
    "\n",
    "#For each iteration:\n",
    "#    *Create 3 models:\n",
    "#       -naive bayes\n",
    "#       -logistic regression\n",
    "#       -decision tree\n",
    "#    *Calculate and save for each model the macro-F1\n",
    "resNaive,resLog,resDes = [],[],[]\n",
    "for train_idx, test_idx in kf.split(X): #aqu√≠ verdaderamente hace split\n",
    "    Xtrain, Ytrain = X[train_idx,:], Y[train_idx]\n",
    "    Xtest, Ytest = X[test_idx,:], Y[test_idx]\n",
    "    #Naive Bayes\n",
    "    model = GaussianNB()\n",
    "    model.fit(Xtrain,Ytrain)\n",
    "    Ypred = model.predict(Xtest)\n",
    "    resNaive.append(f1_score(Ytest,Ypred,average='macro'))\n",
    "    #Logistic Regression\n",
    "    model = LogisticRegression(random_state=0)\n",
    "    model.fit(Xtrain,Ytrain)\n",
    "    Ypred = model.predict(Xtest)\n",
    "    resLog.append(f1_score(Ytest,Ypred,average='macro'))\n",
    "    #Decision Tree\n",
    "    model = DecisionTreeClassifier(random_state=0)\n",
    "    model.fit(Xtrain,Ytrain)\n",
    "    Ypred = model.predict(Xtest)\n",
    "    resDes.append(f1_score(Ytest,Ypred,average='macro'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c80fa28",
   "metadata": {},
   "source": [
    "### Generate a visualization with three boxplots (each one per type of model) showing the values of macro-F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c34d0cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(resNaive)\n",
    "plt.figure()\n",
    "plt.boxplot(resNaive)\n",
    "plt.suptitle('Naive Bayes')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot(resLog)\n",
    "plt.suptitle('Logistic Regression')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot(resDes)\n",
    "plt.suptitle('Decision Tree')\n",
    "plt.show()\n",
    "#resNaive = resLog = resDes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6c70af",
   "metadata": {},
   "source": [
    "### Finally, write a small paragraph analyzing the results describing which one is the best model for this kind of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cc54ac",
   "metadata": {},
   "source": [
    "<sub>After analyzing each model, we can see that the 'Logistic Regression' model presents better results than the others. For example, in the first experiment, its accuracy was the closest to 1 (>9.5), which beat the others, which got an accuracy below 0.9.</sub>\n",
    "\n",
    "<sub>Also, we can see that the 'Naive Bayes' model has the biggest range of the three models. This means it's results vary quite a bit.</sub>\n",
    "\n",
    "<sub>In conclusion, if we could order the models from best to worst it would be:</sub>\n",
    "\n",
    "    \n",
    "      1. Logistic Regression\n",
    "      2. Decision Tree\n",
    "      3. Naive Bayes\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
