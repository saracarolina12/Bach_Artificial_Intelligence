{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pytorch\n",
    "from torch.utils.data import DataLoader, TensorDataset \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1111\n",
    "random.seed(seed) # python seed\n",
    "np.random.seed(seed) #numpy seed\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./mex_train.txt', sep = '\\r\\n', engine = 'python', header = None).loc[:,0].values.tolist()\n",
    "# print(X_train)\n",
    "X_val = pd.read_csv('./mex_val.txt', sep = '\\r\\n', engine = 'python', header = None).loc[:,0].values.tolist()\n",
    "# print(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "\n",
    "class NgramData():\n",
    "    def __init__(self, N: int, vocab_max: int = 5000, tokenizer = None, embeddings_model = None):\n",
    "        self.tokenizer = tokenizer if tokenizer else self.default_tokenizer\n",
    "        self.punct = set(['.', ',', ';', ':', '-', '^', '»', '«', '!', '¡', '¿', '?', '\"', '\\'', '...', '<url>', '*', '@usuario'])\n",
    "        # Orden de modelo\n",
    "        self.N = N\n",
    "        self.vocab_max = vocab_max\n",
    "        self.UNK = '<unk>'\n",
    "        self.SOS = '<s>'\n",
    "        self.EOS = '</s>'\n",
    "        self.embeddings_model = embeddings_model\n",
    "\n",
    "    def get_vocab_size(self) -> list:\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def default_tokenizer(self, doc: str) -> list:\n",
    "        return doc.split(\" \")\n",
    "\n",
    "    def remove_word(self, word: str) -> bool:\n",
    "        word = word.lower()\n",
    "        is_punct = True if word in self.punct else False\n",
    "        is_digit = word.isnumeric()\n",
    "        return is_punct or is_digit\n",
    "\n",
    "    def get_vocab(self, corpus: list) -> set:\n",
    "        freq_dist = FreqDist([w.lower() for sentence in corpus\\\n",
    "                                        for w in self.tokenizer(sentence)\\\n",
    "                                        if not self.remove_word(w)])\n",
    "        sorted_words = self.sortFreqDict(freq_dist)[:self.vocab_max-3]\n",
    "        return set(sorted_words)\n",
    "\n",
    "    def sortFreqDict(self, freq_dist) -> list:\n",
    "        freq_dict = dict(freq_dist)\n",
    "        return sorted(freq_dict, key = freq_dict.get, reverse = True)\n",
    "\n",
    "    def fit(self, corpus: list) -> None:\n",
    "        self.vocab = self.get_vocab(corpus)\n",
    "        self.vocab.add(self.UNK)\n",
    "        self.vocab.add(self.SOS)\n",
    "        self.vocab.add(self.EOS)\n",
    "        self.w2id = {}\n",
    "        self.id2w = {}\n",
    "\n",
    "        if self.embeddings_model is not None:\n",
    "            self.embedding_matrix = np.empty([len(self.vocab), self.embeddings_model.vector_size])\n",
    "\n",
    "        id = 0\n",
    "        for doc in corpus:\n",
    "            for word in self.tokenizer(doc):\n",
    "                word_ = word.lower()\n",
    "                if word_ in self.vocab and not word_ in self.w2id:\n",
    "                    self.w2id[word_] = id\n",
    "                    self.id2w[id] = word_\n",
    "\n",
    "                    if self.embeddings_model is not None:\n",
    "                        if word_ in self.embeddings_model:\n",
    "                            self.embedding_matrix[id] = self.embeddings_model[word_]\n",
    "                        else:\n",
    "                            self.embedding_matrix[id] = np.random.rand(self.embeddings_model[word_].vector_size)\n",
    "                    \n",
    "                    id += 1\n",
    "\n",
    "        self.w2id.update({self.UNK: id, self.SOS: id+1, self.EOS: id+2})\n",
    "        self.id2w.update({id: self.UNK, id+1: self.SOS, id+2: self.EOS})\n",
    "\n",
    "    def transform(self, corpus: list) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X_ngrams=[]\n",
    "        y=[]\n",
    "\n",
    "        for doc in corpus:\n",
    "            doc_ngram = self.get_ngram_doc(doc)\n",
    "            for words_window in doc_ngram:\n",
    "                words_window_ids = [self.w2id[w] for w in words_window]\n",
    "                X_ngrams.append(list(words_window_ids[:-1]))\n",
    "                y.append(words_window_ids[-1])\n",
    "        return np.array(X_ngrams), np.array(y)\n",
    "                \n",
    "    def get_ngram_doc(self, doc: str) -> list:\n",
    "        doc_tokens = self.tokenizer(doc)\n",
    "        doc_tokens = self.replace_unk(doc_tokens)\n",
    "        doc_tokens = [w.lower() for w in doc_tokens]\n",
    "        doc_tokens = [self.SOS] * (self.N-1) + doc_tokens + [self.EOS]\n",
    "        return list(ngrams(doc_tokens, self.N))\n",
    "\n",
    "    def replace_unk(self, doc_tokens: list) -> list:\n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            if token.lower() not in self.vocab:\n",
    "                doc_tokens[i] = self.UNK\n",
    "        return doc_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TweetTokenizer()\n",
    "ngram_data = NgramData(args.N, 5000, tk.tokenize)\n",
    "ngram_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 5000\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocab size: {ngram_data.get_vocab_size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4998, 4998, 4998],\n",
       "       [4998, 4998,    0],\n",
       "       [4998,    0,    1],\n",
       "       ...,\n",
       "       [  20, 4353,  138],\n",
       "       [4353,  138,  163],\n",
       "       [ 138,  163, 4997]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ngram_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ...,  163, 4997, 4999])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ngram_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training observations: (106964, 3), y: (106964,)\n",
      "Validation observations: (11594, 3), y: (11594,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Training observations: {X_ngram_train.shape}, y: {y_ngram_train.shape}')\n",
    "print(f'Validation observations: {X_ngram_val.shape}, y: {y_ngram_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', '<s>', '<s>'],\n",
       " ['<s>', '<s>', 'lo'],\n",
       " ['<s>', 'lo', 'peor'],\n",
       " ['lo', 'peor', 'de'],\n",
       " ['peor', 'de', 'todo'],\n",
       " ['de', 'todo', 'es'],\n",
       " ['todo', 'es', 'que'],\n",
       " ['es', 'que', 'no'],\n",
       " ['que', 'no', 'me'],\n",
       " ['no', 'me', 'dan'],\n",
       " ['me', 'dan', 'por'],\n",
       " ['dan', 'por', 'un'],\n",
       " ['por', 'un', 'tiempo'],\n",
       " ['un', 'tiempo', 'y'],\n",
       " ['tiempo', 'y', 'luego'],\n",
       " ['y', 'luego', 'vuelven'],\n",
       " ['luego', 'vuelven', 'estoy'],\n",
       " ['vuelven', 'estoy', 'hasta'],\n",
       " ['estoy', 'hasta', 'la'],\n",
       " ['hasta', 'la', 'verga'],\n",
       " ['la', 'verga', 'de'],\n",
       " ['verga', 'de', 'estl']]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[ngram_data.id2w[w] for w in tw] for tw in X_ngram_train[:22]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lo',\n",
       " 'peor',\n",
       " 'de',\n",
       " 'todo',\n",
       " 'es',\n",
       " 'que',\n",
       " 'no',\n",
       " 'me',\n",
       " 'dan',\n",
       " 'por',\n",
       " 'un',\n",
       " 'tiempo',\n",
       " 'y',\n",
       " 'luego',\n",
       " 'vuelven',\n",
       " 'estoy',\n",
       " 'hasta',\n",
       " 'la',\n",
       " 'verga',\n",
       " 'de',\n",
       " 'estl',\n",
       " '</s>']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ngram_data.id2w[w] for w in y_ngram_train[:22]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 64\n",
    "args.num_workers = 2\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype = torch.int64), torch.tensor(y_ngram_train, dtype = torch.int64))\n",
    "train_loader = DataLoader(train_dataset, batch_size = args.batch_size, num_workers = args.num_workers, shuffle = True)\n",
    "\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype = torch.int64), torch.tensor(y_ngram_val, dtype = torch.int64))\n",
    "val_loader = DataLoader(val_dataset, batch_size = args.batch_size, num_workers = args.num_workers, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 3])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(f'X shape: {batch[0].shape}')\n",
    "print(f'y shape: {batch[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4997, 4997,  703],\n",
       "        [1659,   86,  114],\n",
       "        [ 121, 4997, 4997]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][:3] # Piece of batch of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4999,  257, 4997])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1][:3] # Piece of batch of the training 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "args.droput = 0.1\n",
    "# Dimensions\n",
    "args.d = 50\n",
    "# Dimension for hidden layer\n",
    "args.d_h = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(NeuralLM, self).__init__()\n",
    "\n",
    "        self.window_size = args.N-1\n",
    "        self.embedding_dim = args.d\n",
    "\n",
    "        self.emb = nn.Embedding(args.vocab_size, args.d)\n",
    "        self.fc1 = nn.Linear(args.d * (args.N-1), args.d_h)\n",
    "        self.drop1 = nn.Dropout(p=args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.window_size * self.embedding_dim)\n",
    "        # Tanh\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop1(h)\n",
    "        return self.fc2(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7562, -0.2647, -0.9040],\n",
       "        [ 0.3331, -0.9770, -1.7632],\n",
       "        [-0.0122, -1.2013,  0.5056]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = nn.Embedding(10, 3)\n",
    "e.weight\n",
    "e(torch.tensor([0, 2, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(raw_logits):\n",
    "    probs = F.softmax(raw_logits.detach(), dim = 1)\n",
    "    y_pred = torch.argmax(probs, dim = 1).cpu().numpy()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(data, model, gpu = False):\n",
    "    with torch.no_grad():\n",
    "        preds, tgts = [], []\n",
    "        for window_words, labels in data:\n",
    "            if gpu: window_words = window_words.cuda()\n",
    "            \n",
    "            outputs = model(window_words)\n",
    "            y_pred  = get_preds(outputs)\n",
    "            tgt = labels.numpy()\n",
    "            tgts.append(tgt)\n",
    "            preds.append(y_pred)\n",
    "    \n",
    "    tgts = [e for l in tgts for e in l]\n",
    "    preds = [e for l in preds for e in l]\n",
    "\n",
    "    return accuracy_score(tgts, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint_path, filename = 'checkpoint.pt'):\n",
    "    filename = os.path.join(checkpoint_path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, os.path.join(checkpoint_path, 'model_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "args.d = 100\n",
    "args.d_h = 200\n",
    "args.dropout = 0.1\n",
    "# Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20 # Early stopping, how many epochs we wait without changes\n",
    "# Scheduler hyperparameters\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "# Saving directory\n",
    "args.savedir = 'model'\n",
    "os.makedirs(args.savedir, exist_ok = True)\n",
    "# Create model\n",
    "model = NeuralLM(args)\n",
    "#Send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "# Lost, Optimizer and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience = args.lr_patience, verbose = True, factor = args.lr_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\t Loss: 5.5151 - Val accuracy: 0.2190 - Epoch time: 42.65\n",
      "\tTrain acc: 0.16504560406698565\n",
      "Epoch [2/100]\t Loss: 5.0724 - Val accuracy: 0.1706 - Epoch time: 41.84\n",
      "\tTrain acc: 0.17738113038277512\n",
      "Epoch [3/100]\t Loss: 4.8623 - Val accuracy: 0.2224 - Epoch time: 42.38\n",
      "\tTrain acc: 0.18495813397129188\n",
      "Epoch [4/100]\t Loss: 4.6961 - Val accuracy: 0.1757 - Epoch time: 43.48\n",
      "\tTrain acc: 0.19127168062200955\n",
      "Epoch [5/100]\t Loss: 4.5484 - Val accuracy: 0.1907 - Epoch time: 42.95\n",
      "\tTrain acc: 0.19384531997607657\n",
      "Epoch [6/100]\t Loss: 4.4192 - Val accuracy: 0.1475 - Epoch time: 43.20\n",
      "\tTrain acc: 0.1961666417464115\n",
      "Epoch [7/100]\t Loss: 4.2943 - Val accuracy: 0.1253 - Epoch time: 42.84\n",
      "\tTrain acc: 0.19916641746411484\n",
      "Epoch [8/100]\t Loss: 4.1753 - Val accuracy: 0.1846 - Epoch time: 39.82\n",
      "\tTrain acc: 0.20241851076555026\n",
      "Epoch [9/100]\t Loss: 4.0700 - Val accuracy: 0.2094 - Epoch time: 40.83\n",
      "\tTrain acc: 0.20569116327751194\n",
      "Epoch [10/100]\t Loss: 3.9687 - Val accuracy: 0.1347 - Epoch time: 40.65\n",
      "\tTrain acc: 0.20790408193779905\n",
      "Epoch [11/100]\t Loss: 3.8715 - Val accuracy: 0.1313 - Epoch time: 39.96\n",
      "\tTrain acc: 0.21469235944976076\n",
      "Epoch [12/100]\t Loss: 3.7762 - Val accuracy: 0.2038 - Epoch time: 43.21\n",
      "\tTrain acc: 0.22140026913875596\n",
      "Epoch [13/100]\t Loss: 3.6849 - Val accuracy: 0.1283 - Epoch time: 40.04\n",
      "\tTrain acc: 0.22945574162679425\n",
      "Epoch [14/100]\t Loss: 3.6058 - Val accuracy: 0.2152 - Epoch time: 39.18\n",
      "\tTrain acc: 0.23894475179425836\n",
      "Epoch [15/100]\t Loss: 3.5415 - Val accuracy: 0.1785 - Epoch time: 39.24\n",
      "\tTrain acc: 0.2456806967703349\n",
      "Epoch [16/100]\t Loss: 3.4824 - Val accuracy: 0.1185 - Epoch time: 38.08\n",
      "\tTrain acc: 0.25443518241626795\n",
      "Epoch [17/100]\t Loss: 3.4166 - Val accuracy: 0.2100 - Epoch time: 41.75\n",
      "\tTrain acc: 0.26454096889952156\n",
      "Epoch [18/100]\t Loss: 3.3680 - Val accuracy: 0.1647 - Epoch time: 39.84\n",
      "\tTrain acc: 0.269761139354067\n",
      "Epoch [19/100]\t Loss: 3.3123 - Val accuracy: 0.1871 - Epoch time: 39.12\n",
      "\tTrain acc: 0.27926510167464114\n",
      "Epoch [20/100]\t Loss: 3.2641 - Val accuracy: 0.1661 - Epoch time: 40.02\n",
      "\tTrain acc: 0.28613748504784686\n",
      "Epoch [21/100]\t Loss: 3.2242 - Val accuracy: 0.2004 - Epoch time: 40.53\n",
      "\tTrain acc: 0.2921426435406698\n",
      "Epoch [22/100]\t Loss: 3.1920 - Val accuracy: 0.1684 - Epoch time: 42.51\n",
      "\tTrain acc: 0.2968039772727273\n",
      "No improvement. Breaking out of loop.\n",
      "--- 942.8272166252136 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "\n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "        outputs = model(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader, model, gpu = args.use_gpu)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    scheduler.step(tuning_metric)\n",
    "\n",
    "    is_improvement = tuning_metric > best_metric\n",
    "    if is_improvement:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "    save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"best_metric\": best_metric,\n",
    "        },\n",
    "        is_improvement,\n",
    "        args.savedir,\n",
    "    )\n",
    "\n",
    "    # Early Stopping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print(\"No improvement. Breaking out of loop.\")\n",
    "        break\n",
    "\n",
    "    print('Epoch [{}/{}]\\t Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}'.format(epoch + 1, args.num_epochs, np.mean(loss_epoch), tuning_metric, (time.time() - epoch_start_time))) \n",
    "    print(\"\\tTrain acc: {}\".format(mean_epoch_metric))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(embeddings, ngram_data, word, n):\n",
    "    word_id = torch.LongTensor([ngram_data.w2id[word]])\n",
    "    word_embed = embeddings(word_id)\n",
    "    dists = torch.norm(embeddings.weight - word_embed, dim = 1).detach()\n",
    "    lst = sorted(enumerate(dists.numpy()), key = lambda x: x[1])\n",
    "    for idx, difference in lst[1: n + 1]:\n",
    "        print(ngram_data.id2w[idx], difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned Embeddings\n",
      "------------------------------\n",
      "sabor 11.239676\n",
      "celu 11.2588215\n",
      "putero 11.748031\n",
      "fake 11.812265\n",
      "mierdas 11.831186\n",
      "patriarcado 11.88779\n",
      "comida 11.9002075\n",
      "sin 11.905333\n",
      "caes 11.9176855\n",
      "ver 11.943648\n"
     ]
    }
   ],
   "source": [
    "best_model = NeuralLM(args)\n",
    "best_model.load_state_dict(torch.load('model/model_best.pt')['state_dict'])\n",
    "best_model.train(False)\n",
    "\n",
    "print('-' * 30)\n",
    "print(\"Learned Embeddings\")\n",
    "print('-' * 30)\n",
    "print_closest_words(best_model.emb, ngram_data, 'jaja', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text, tokenizer):\n",
    "    all_tokens = [w.lower() if w in ngram_data.w2id else '<unk>' for w in tokenizer.tokenize(text)]\n",
    "    token_ids = [ngram_data.w2id[word.lower()] for word in all_tokens]\n",
    "    return all_tokens, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_word(logits, temperature = 1.0):\n",
    "    logits = np.asarray(logits).astype('float64')\n",
    "    preds = logits/temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds/np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(model, token_ids):\n",
    "    word_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
    "    y_raw_pred = model(word_ids_tensor).squeeze(0).detach().numpy()\n",
    "    y_pred = sample_next_word(y_raw_pred, 1.0)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, initial_text, tokenizer):\n",
    "    all_tokens, window_word_ids = parse_text(initial_text, tokenizer)\n",
    "    for i in range(100):\n",
    "        y_pred = predict_next_token(best_model, window_word_ids)\n",
    "        next_word = ngram_data.id2w[y_pred]\n",
    "        all_tokens.append(next_word)\n",
    "\n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "        else:\n",
    "            window_word_ids.pop(0)\n",
    "            window_word_ids.append(y_pred)\n",
    "    return \" \".join(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> <s> <s> pinche <unk> para estas <unk> <unk> > que no le importa <unk> <unk> inútil tatuajes menos <unk> <unk> viviendo comprarme pero <unk> en los putos periodistas <unk> <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = '<s> <s> <s>'\n",
    "print('-' * 30)\n",
    "print('Learned embeddings')\n",
    "print('-' * 30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> <s> estoy hasta la <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = '<s> <s> estoy'\n",
    "print('-' * 30)\n",
    "print('Learned embeddings')\n",
    "print('-' * 30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> saludos a mí <unk> deja <unk> <unk> <unk> o de <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = '<s> saludos a'\n",
    "print('-' * 30)\n",
    "print('Learned embeddings')\n",
    "print('-' * 30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "yo opino que así de <unk> <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = 'yo opino que'\n",
    "print('-' * 30)\n",
    "print('Learned embeddings')\n",
    "print('-' * 30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(model, text, ngram_model):\n",
    "    X, y = ngram_data.transform([text])\n",
    "    X, y = X[2: ], y[2: ]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "\n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim = 1).numpy()\n",
    "\n",
    "    return np.sum([np.log(probs[i][w]) for i, w in enumerate(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -18.802128\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Estamos en la clase de procesamiento de lenguaje\", ngram_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -28.936174\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", log_likelihood(best_model, \"Estamos procesamiento clase en la de natural lenguaje\", ngram_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -43.338364\n"
     ]
    }
   ],
   "source": [
    "print( \"log likelihood: \", log_likelihood(best_model, \"la natural Estamos clase en de de lenguaje procesamiento\", ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estructuras Sintácticas Correctas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "-19.254353 sino gano me voy a la chingada\n",
      "-20.071653 gano sino me voy a la chingada\n",
      "-24.87542 sino gano a la chingada me voy\n",
      "-25.02592 gano sino a la chingada me voy\n",
      "-26.311047 gano sino voy a la chingada me\n",
      "--------------------------------------------------\n",
      "-56.92278 chingada me a sino voy gano la\n",
      "-57.021553 a la voy gano chingada sino me\n",
      "-57.21825 me la voy gano chingada sino a\n",
      "-57.32174 me a sino voy gano chingada la\n",
      "-59.04985 me a chingada sino voy gano la\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "word_list = \"sino gano me voy a la chingada\".split(' ')\n",
    "perms = [' '.join(perm) for perm in permutations(word_list)]\n",
    "print('-' * 50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse = True)[: 5]:\n",
    "    print(p, t)\n",
    "print('-' * 50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse = True)[-5:]:\n",
    "    print(p, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
