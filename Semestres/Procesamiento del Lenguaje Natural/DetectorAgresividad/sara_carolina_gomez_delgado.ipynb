{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 4\n",
    "0226594 || Sara Carolina Gómez Delgado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_from_file(path_corpus, path_truth):\n",
    "    tr_txt = []\n",
    "    tr_y = []\n",
    "    with open(path_corpus, \"r\",encoding=\"utf8\") as f_corpus, open(path_truth, \"r\",encoding=\"utf8\") as f_truth:\n",
    "        for tuit in f_corpus:\n",
    "            tr_txt += [tuit]\n",
    "        for label in f_truth:\n",
    "            tr_y += [label] \n",
    "    return tr_txt, tr_y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modelos de Lenguaje y Evaluación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Preprocesamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucción</kbd>_\n",
    "\n",
    "Preprocese todos los tuits de agresividad (positivos y negativos) según su intuición para construir un buen corpus para un modelo de lenguaje (e.g., solo palabras en minúscula, etc.). Agregue tokens especiales de < s > y </ s > según usted considere (e.g., al\n",
    "inicio y final de cada tuit). Defina su vocabulario y enmascare con < unk > toda palabra\n",
    "que no esté en su vocabulario.\n",
    "\n",
    "\n",
    "_<kbd>Comentario</kbd>_\n",
    "\n",
    "En esta sección se preprocesaron los tweets y se generó un corpus donde sólo se consideraron palabras (no emojis, no signos de puntuación) y se eligió un vocabulario tomando las palabras únicas en el training set, después se ordenaron según su frecuencia (mayor a menor) y se tomaron sólo las palabras cuya frecuencia fue mayor a 50 para formar el vocabulario final. Me parece importante resaltar, como principal conclusión, la importancia de descartar palabras que aparecen poco, ya que pueden llegar a afectar fuertemente al intentar aplicar un modelo de lenguaje."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Leer tweets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_txt, tr_y = get_texts_from_file(\"./mex_train.txt\", \"./mex_train_labels.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Pre-procesar tweets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TweetTokenizer instance\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <s> at the beggining of a tweet and </s> at the end of this tweet\n",
    "corpus_palabras = []\n",
    "for doc in tr_txt:\n",
    "    x = \"<s>\" + doc + \"</s>\"\n",
    "    corpus_palabras += tokenizer.tokenize(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'lo', 'peor', 'de', 'todo', 'es', 'que', 'no', 'me', 'dan']\n"
     ]
    }
   ],
   "source": [
    "processed = []\n",
    "for word in corpus_palabras:\n",
    "    if(np.char.isalpha(word) or word == \"<s>\" or word == \"</s>\"):\n",
    "            processed.append(word)\n",
    "print(processed[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(processed) # frecuencia de cada palabra\n",
    "\n",
    "def sortFreqDict(freqdict):\n",
    "    aux = [(freqdict[key], key) for key in freqdict] #lista de pares ordenada (más frecuente a menos)\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    return aux #regresa el objeto ordenado en reversa (más frecuentes a menos frecuentes)\n",
    "\n",
    "V = sortFreqDict(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', 'que', 'de', 'a', 'la', 'y', 'no', 'me', 'el']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if freq < 50, ignore it (don't let it be part of vocab)\n",
    "vocab = []\n",
    "for item in V:\n",
    "    if item[0] > 50:\n",
    "        vocab.append(item[1])\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'lo', '<unk>', 'de', 'todo', 'es', 'que', 'no', 'me', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "for i,word in enumerate(processed):\n",
    "    if word not in vocab:\n",
    "        processed[i] = \"<unk>\"\n",
    "print(processed[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Entrenamiento (unigrama, bigrama y trigrama)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucción</kbd>_\n",
    "\n",
    "Entrene tres modelos de lenguaje sobre todos los tuis: $P_{unigramas}(w^n_1), P_{bigramas}(w^n_1), P{trigramas}(w^n_1).$ Para cada uno proporcione  una interfaz (función) sencilla para $P{n-grama}(w^n_1)$ y $Pn-grama(w^n_1 | w^{n-1}_{n-N+1})$. Los modelos modelos deben tener una estrategia común para lidiar con\n",
    "secuencias no vistas. Puede optar por un suavizamiento Laplace o un Good-Turing\n",
    "discounting. Muestre un par de ejemplos de como funciona, al menos uno con una\n",
    "palabra fuera del vocabulario."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Construcción de Modelo Interpolado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucción</kbd>_\n",
    "\n",
    "Construya un modelo interpolado con valores $\\lambda$ fijos:\n",
    "\n",
    "$$\\hat{P}(w_n|w_{n-2}w_{n-1}) = \\lambda_1P(w_n|w_{n-2}w_{n-1}) + \\lambda_2P(w_n|w_{n-1}) + \\lambda_3P(w_n)$$\n",
    "\n",
    "Para ello experimente con el modelo en particiones estratificadas de 80%, 10% y 10% para\n",
    "entrenar (train), ajuste de parámetros (val) y prueba (test) respectivamente. Muestre como\n",
    "bajan o suben las perplejidades en validación, finalmente pruebe una vez en test. Para esto puede explorar algunos valores ⃗λ y elija el mejor, i.e., [1/3, 1/3, 1/3],[.4, .4, .2],[.2, .4, .4],[.5, .4, .1]\n",
    "y [.1, .4, .5]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generación de Texto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta parte reentrenará su modelo de lenguaje interpolado para aprender los valores λ:\n",
    "\n",
    "$$\\hat{P}(w_n|w_{n-2}w_{n-1}) = \\lambda_1P(w_n|w_{n-2}w_{n-1}) + \\lambda_2P(w_n|w_{n-1}) + \\lambda_3P(w_n)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Función \"tuitear\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucción</kbd>_\n",
    "\n",
    "Haga una función \"tuitear\" con base en su modelo de lenguaje P̂ del último punto.\n",
    "El modelo deberá poder parar automáticamente cuando genere el símbolo de terminación\n",
    "de tuit al final (e.g., \"</s>\"), o 50 palabras. Proponga algo para que en los últimos tokens\n",
    "sea más probable generar el token \"</s>\". Muestre al menos cinco ejemplos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Entrenar modelo de lenguaje AMLO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucción</kbd>_\n",
    "\n",
    "Use la intuición que ha ganado en esta tarea y los datos de las mañaneras para\n",
    "entrenar un modelo de lenguaje AMLO. Haga una un función \"dar_conferencia()\". Generé\n",
    "un discurso de 300 palabras y detenga al modelo de forma abrupta."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Experimentando con dos frases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucción</kbd>_\n",
    "\n",
    "Calcule el estimado de cada uno sus modelos de lenguaje (el de tuits y el de amlo)\n",
    "para las frases: \"sino gano me voy a la chingada\", \"ya se va a acabar la corrupción\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Permutaciones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucción</kbd>_\n",
    "\n",
    "Para cada oración del punto anterior, haga todas las permutaciones posibles.\n",
    "Calcule su probabilidad a cada nueva frase y muestre el top 3 mas probable y el top 3\n",
    "menos probable (para ambos modelos de lenguaje). Proponga una frase más y haga lo\n",
    "mismo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
