{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 4\n",
    "0226594 || Sara Carolina Gómez Delgado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from itertools import islice\n",
    "import random\n",
    "import mpmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_from_file(path_corpus, path_truth):\n",
    "    tr_txt = []\n",
    "    tr_y = []\n",
    "    with open(path_corpus, \"r\",encoding=\"utf8\") as f_corpus, open(path_truth, \"r\",encoding=\"utf8\") as f_truth:\n",
    "        for tuit in f_corpus:\n",
    "            tr_txt += [tuit]\n",
    "        for label in f_truth:\n",
    "            tr_y += [label] \n",
    "    return tr_txt, tr_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Calculate Perplexity_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(prob, N): \n",
    "    inv = 1/prob\n",
    "    return mpmath.root(inv, N)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h1 style=\"text-align: center;\">Parte 1</h1>\n",
    "\n",
    "<h4 style=\"text-align: center;\"><i>\"Modelos de Lenguaje y Evaluación\"</i></h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Preprocesamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucción</kbd>_\n",
    "\n",
    "Preprocese todos los tuits de agresividad (positivos y negativos) según su intuición para construir un buen corpus para un modelo de lenguaje (e.g., solo palabras en minúscula, etc.). Agregue tokens especiales de < s > y </ s > según usted considere (e.g., al\n",
    "inicio y final de cada tuit). Defina su vocabulario y enmascare con < unk > toda palabra\n",
    "que no esté en su vocabulario.\n",
    "\n",
    "\n",
    "_<kbd>Comentario</kbd>_\n",
    "\n",
    "En esta sección se preprocesaron los tweets y se generó un corpus donde sólo se consideraron palabras (no emojis, no signos de puntuación) y se eligió un vocabulario tomando las palabras únicas en el training set, después se ordenaron según su frecuencia (mayor a menor) y se tomaron sólo las palabras cuya frecuencia fue mayor a 50 para formar el vocabulario final. Me parece importante resaltar, como principal conclusión, la importancia de descartar palabras que aparecen poco, ya que pueden llegar a afectar fuertemente al intentar aplicar un modelo de lenguaje."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Leer tweets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_txt, tr_y = get_texts_from_file(\"./mex_train.txt\", \"./mex_train_labels.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Pre-procesar tweets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TweetTokenizer instance\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <s> at the beggining of a tweet and </s> at the end of this tweet\n",
    "corpus_palabras = []\n",
    "for doc in tr_txt:\n",
    "    x = \"<s>\" + doc + \"</s>\"\n",
    "    corpus_palabras += tokenizer.tokenize(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'lo', 'peor', 'de', 'todo', 'es', 'que', 'no', 'me', 'dan']\n"
     ]
    }
   ],
   "source": [
    "processed = []\n",
    "for word in corpus_palabras:\n",
    "    if(np.char.isalpha(word) or word == \"<s>\" or word == \"</s>\"):\n",
    "            processed.append(word)\n",
    "print(processed[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(processed) # frecuencia de cada palabra\n",
    "def sortFreqDict(freqdict):\n",
    "    aux = [(freqdict[key], key) for key in freqdict] #lista de pares ordenada (más frecuente a menos)\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    return aux #regresa el objeto ordenado en reversa (más frecuentes a menos frecuentes)\n",
    "\n",
    "V = sortFreqDict(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', 'que', 'de', 'a', 'la', 'y', 'no', 'me', 'el']"
      ]
     },
     "execution_count": 1015,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if freq < 50, ignore it (don't let it be part of vocab)\n",
    "vocab = []\n",
    "for item in V:\n",
    "    if item[0] > 50:\n",
    "        vocab.append(item[1])\n",
    "vocab[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'lo', '<unk>', 'de', 'todo', 'es', 'que', 'no', 'me', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "for i,word in enumerate(processed):\n",
    "    if word not in vocab:\n",
    "        processed[i] = \"<unk>\"\n",
    "print(processed[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Entrenamiento (unigrama, bigrama y trigrama)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucción</kbd>_\n",
    "\n",
    "Entrene tres modelos de lenguaje sobre todos los tuis: $P_{unigramas}(w^n_1), P_{bigramas}(w^n_1), P{trigramas}(w^n_1).$ Para cada uno proporcione  una interfaz (función) sencilla para $P{n-grama}(w^n_1)$ y $Pn-grama(w^n_1 | w^{n-1}_{n-N+1})$. Los modelos modelos deben tener una estrategia común para lidiar con\n",
    "secuencias no vistas. Puede optar por un suavizamiento Laplace o un Good-Turing\n",
    "discounting. Muestre un par de ejemplos de como funciona, al menos uno con una\n",
    "palabra fuera del vocabulario.\n",
    "\n",
    "\n",
    "_<kbd>Comentario</kbd>_\n",
    "\n",
    "En esta sección decidí crear dos funciones aplicando en ambas Laplace Smoothing (Add-One) para manejar el caso de secuencias no vistas.\n",
    "1) Función que devuelve la probabilidad de que suceda una palabra dada un contexto.\n",
    "2) Función que devuelve la probabilidad de que suceda una oración.\n",
    "\n",
    "Como conclusion, me llamó mucho la atención el hecho de que cuando se encuentra el modelo con una palabra que no ha visto antes, no la vuelve cero (por al add-one de Laplace), si no que su probabilidad se vuelve muy pequeña."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram\n",
      "\t0.0133005624\n",
      "bigram\n",
      "\t0.0013767184\n",
      "trigram\n",
      "\t0.0000404285\n",
      "bigram\n",
      "\t0.0000101145\n"
     ]
    }
   ],
   "source": [
    "def prob_word(words, context, vocab_size): # (list)\n",
    "    assert isinstance(words, list)\n",
    "    assert isinstance(context, list)\n",
    "    assert len(context) > 0\n",
    "    if len(words) == 1: # prob = (word_freq + k) / (context_size + k*V)\n",
    "        print(\"unigram\")\n",
    "        word = words[0]\n",
    "        word_freq = context.count(word)\n",
    "        context_size = len(context)\n",
    "        prob = (word_freq+1)/(context_size+vocab_size)\n",
    "        return prob\n",
    "    elif len(words) == 2: # prob = (bigram_freq + k) / (a_freq + k*V)\n",
    "        print(\"bigram\")\n",
    "        a,b = words\n",
    "        a_freq = context.count(a)\n",
    "        bigram_freq = 0\n",
    "        for i in range(len(context)-1):\n",
    "            if context[i]==a and context[i+1]==b:\n",
    "                bigram_freq += 1\n",
    "        prob = (bigram_freq+1)/(a_freq+vocab_size)\n",
    "        return prob\n",
    "    elif len(words) == 3: # prob = (count_trigram + k) / (count_bigram + k*V)\n",
    "        print(\"trigram\")\n",
    "        count_trigram = 0\n",
    "        count_bigram = 0\n",
    "        for i in range(len(context)-2):\n",
    "            # number of times the trigram appears\n",
    "            if context[i:i+3] == words:\n",
    "                count_trigram += 1\n",
    "            # number of times the first two words of the trigram appear in the context (bigram)\n",
    "            if context[i:i+2] == words[:2]:\n",
    "                count_bigram += 1\n",
    "        prob = (count_trigram+1)/(count_bigram + vocab_size)\n",
    "        return prob\n",
    "\n",
    "# unigrama\n",
    "\n",
    "result = prob_word([\"a\"], processed, len(processed))\n",
    "print(\"\\t{:.10f}\".format(result))\n",
    "\n",
    "# bigrama\n",
    "result = prob_word([\"lo\", \"que\"], processed, len(processed))\n",
    "print(\"\\t{:.10f}\".format(result))\n",
    "\n",
    "# trigrama\n",
    "result = prob_word([\"si\", \"no\", \"fuera\"], processed, len(processed))\n",
    "print(\"\\t{:.10f}\".format(result))\n",
    "\n",
    "# These words don't belong to the vocabulary\n",
    "result = prob_word([\"hello\", \"dude\"], processed, len(processed))\n",
    "print(\"\\t{:.10f}\".format(result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram\n",
      "\t0.0341057935\n"
     ]
    }
   ],
   "source": [
    "def ngram_prob(n, sentence, corpus):\n",
    "    assert isinstance(n, int)\n",
    "    assert isinstance(sentence, list)\n",
    "    assert n <= len(sentence)\n",
    "    assert isinstance(corpus, list)\n",
    "\n",
    "    freq = {}\n",
    "    for i in range(len(corpus)-n+1):\n",
    "        gram = tuple(corpus[i:i+n]) # n-gram (1-3)\n",
    "        if gram in freq:\n",
    "            freq[gram] += 1\n",
    "        else:\n",
    "            freq[gram] = 1\n",
    "    # Laplace smoothing\n",
    "    V = len(set(corpus))\n",
    "    for gram in freq:\n",
    "        freq[gram] += 1\n",
    "    count = 0\n",
    "    for val in freq.values():\n",
    "        count += val\n",
    "    count += V\n",
    "    prob = (freq.get(tuple(sentence[-n:]),0)+1)/count\n",
    "    return prob\n",
    "\n",
    "ngram = 1\n",
    "result = ngram_prob(ngram, ['de', 'todo', 'lo', 'que'], processed)\n",
    "if ngram == 1: print(\"unigram\")\n",
    "elif ngram == 2: print(\"bigram\")\n",
    "elif ngram == 3: print(\"trigram\")\n",
    "else: print(\"n-gram\")\n",
    "print(\"\\t{:.10f}\".format(result))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Construcción de Modelo Interpolado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucción</kbd>_\n",
    "\n",
    "Construya un modelo interpolado con valores $\\lambda$ fijos:\n",
    "\n",
    "$$\\hat{P}(w_n|w_{n-2}w_{n-1}) = \\lambda_1P(w_n|w_{n-2}w_{n-1}) + \\lambda_2P(w_n|w_{n-1}) + \\lambda_3P(w_n)$$\n",
    "\n",
    "Para ello experimente con el modelo en particiones estratificadas de 80%, 10% y 10% para\n",
    "entrenar (train), ajuste de parámetros (val) y prueba (test) respectivamente. Muestre como\n",
    "bajan o suben las perplejidades en validación, finalmente pruebe una vez en test. Para esto puede explorar algunos valores ⃗λ y elija el mejor, i.e., [1/3, 1/3, 1/3],[.4, .4, .2],[.2, .4, .4],[.5, .4, .1]\n",
    "y [.1, .4, .5].\n",
    "\n",
    "_<kbd>Comentario</kbd>_\n",
    "\n",
    "En esta sección dividí primero mis datos en 80% train, 10% validation y 10% test. Después, creé el modelo interpolado donde $\\lambda_1P(w_n|w_{n-2}w_{n-1})$ representa un trigrama multiplicado por un valor lambda_1, \\lambda_2P(w_n|w_{n-1}) representa un bigrama multiplicado por un valor de lambda_2 y \\lambda_3P(w_n) representa un unigrama multiplicado por un valor de lambda_3. Los resultados son sumados. Sumé estos valores y finalmente experimenté con los diferentes valores de lambda para comparar sus perplejidades y quedarme con el valor más pequeño."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Data Partition (80% (train), 10% validation, 10% test)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  79.99959542015617\n",
      "Validation:  10.000202289921916\n",
      "Test:  10.000202289921916\n"
     ]
    }
   ],
   "source": [
    "# Shuffle data\n",
    "rand_data = processed.copy()\n",
    "random.shuffle(rand_data)\n",
    "\n",
    "\n",
    "# Divide 80% Train, 20% Test\n",
    "n = len(rand_data) \n",
    "n_train = int(n * 0.8)  # entrenamiento (80%)\n",
    "n_test = n - n_train  # prueba (20%)\n",
    "train = rand_data[:n_train]\n",
    "to_divide = rand_data[n_train:]\n",
    "\n",
    "# Divide that 20% in two parts (10% Validation, 10% Test)\n",
    "n_test = len(to_divide) \n",
    "n_val = n_test//2  # Validation (50%)\n",
    "n_test = n_test - n_val  # Test(50%)\n",
    "validation = to_divide[:n_val]\n",
    "test = to_divide[n_val:]\n",
    "\n",
    "print(\"Train: \",(len(train)*100)/len(processed))\n",
    "print(\"Validation: \",(len(validation)*100)/len(processed))\n",
    "print(\"Test: \",(len(test)*100)/len(processed))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Find Best Lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_lambda(uni, bi, tri, lambdas, N):\n",
    "    assert len(lambdas) >= 1\n",
    "    best_lam = lambdas[0]\n",
    "    best_perplexity = 10000\n",
    "    for l in lambdas:\n",
    "        probability = uni*l[0] + bi*l[1] + tri*l[2]\n",
    "        p = perplexity(probability, N)\n",
    "        if(p < best_perplexity):\n",
    "            best_perplexity = p\n",
    "            best_lam = [l[0], l[1], l[2]]\n",
    "    return best_lam, best_perplexity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Validation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram: 0.0008304394\n",
      "Bigram: 0.0000448355\n",
      "Trigram: 0.0000087011\n",
      "best lambda and its perplexity:  [0.5, 0.4, 0.1] 1.00078339668382\n"
     ]
    }
   ],
   "source": [
    "lambdas = [[1/3, 1/3, 1/3], [.4, .4, .2], [.2, .4, .4], [.5, .4, .1], [.1, .4, .5]]\n",
    "\n",
    "# unigram\n",
    "ngram = 1\n",
    "uni = ngram_prob(ngram, validation, train)\n",
    "print(\"Unigram: {:.10f}\".format(uni))\n",
    "\n",
    "# bigram\n",
    "ngram = 2\n",
    "bi = ngram_prob(ngram, validation, train)\n",
    "print(\"Bigram: {:.10f}\".format(bi))\n",
    "\n",
    "# trigram\n",
    "ngram = 3\n",
    "tri = ngram_prob(ngram, validation, train)\n",
    "print(\"Trigram: {:.10f}\".format(tri))\n",
    "\n",
    "res = best_lambda(uni,bi,tri, lambdas, len(validation))\n",
    "validation_best_lambda = res[0]\n",
    "validation_best_perplexity = res[1]\n",
    "print(\"best lambda and its perplexity: \",validation_best_lambda, validation_best_perplexity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram: 0.0244098847\n",
      "Bigram: 0.0073306058\n",
      "Trigram: 0.0000087011\n",
      "best lambda and its perplexity:  [0.5, 0.4, 0.1] 1.00042393362114\n"
     ]
    }
   ],
   "source": [
    "lambdas = [[1/3, 1/3, 1/3], [.4, .4, .2], [.2, .4, .4], [.5, .4, .1], [.1, .4, .5]]\n",
    "\n",
    "# unigram\n",
    "ngram = 1\n",
    "uni = ngram_prob(ngram, test , train)\n",
    "print(\"Unigram: {:.10f}\".format(uni))\n",
    "\n",
    "# bigram\n",
    "ngram = 2\n",
    "bi = ngram_prob(ngram, test , train)\n",
    "print(\"Bigram: {:.10f}\".format(bi))\n",
    "\n",
    "# trigram\n",
    "ngram = 3\n",
    "tri = ngram_prob(ngram, test , train)\n",
    "print(\"Trigram: {:.10f}\".format(tri))\n",
    "\n",
    "res = best_lambda(uni,bi,tri, lambdas, len(test))\n",
    "test_best_lambda = res[0]\n",
    "test_best_perplexity = res[1]\n",
    "print(\"best lambda and its perplexity: \", test_best_lambda, test_best_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Show Best Perplexity_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Perplexity (test):  1.00042393362114\n",
      "Lambdas:  [0.5, 0.4, 0.1]\n"
     ]
    }
   ],
   "source": [
    "if validation_best_perplexity < test_best_perplexity:\n",
    "    print(\"Best Perplexity (validation): \", validation_best_perplexity)\n",
    "    print(\"Lambdas: \", validation_best_lambda)\n",
    "else:\n",
    "    print(\"Best Perplexity (test): \", test_best_perplexity)\n",
    "    print(\"Lambdas: \", test_best_lambda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h1 style=\"text-align: center;\">Parte 2</h1>\n",
    "\n",
    "<h4 style=\"text-align: center;\"><i>\"Generación de Texto\"</i></h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta parte reentrenará su modelo de lenguaje interpolado para aprender los valores λ:\n",
    "\n",
    "$$\\hat{P}(w_n|w_{n-2}w_{n-1}) = \\lambda_1P(w_n|w_{n-2}w_{n-1}) + \\lambda_2P(w_n|w_{n-1}) + \\lambda_3P(w_n)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Función \"tuitear\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucción</kbd>_\n",
    "\n",
    "Haga una función \"tuitear\" con base en su modelo de lenguaje P̂ del último punto.\n",
    "El modelo deberá poder parar automáticamente cuando genere el símbolo de terminación\n",
    "de tuit al final (e.g., \"</s>\"), o 50 palabras. Proponga algo para que en los últimos tokens\n",
    "sea más probable generar el token \"</s>\". Muestre al menos cinco ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuitear():\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Entrenar modelo de lenguaje AMLO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucción</kbd>_\n",
    "\n",
    "Use la intuición que ha ganado en esta tarea y los datos de las mañaneras para\n",
    "entrenar un modelo de lenguaje AMLO. Haga una un función \"dar_conferencia()\". Generé\n",
    "un discurso de 300 palabras y detenga al modelo de forma abrupta."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Experimentando con dos frases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucción</kbd>_\n",
    "\n",
    "Calcule el estimado de cada uno sus modelos de lenguaje (el de tuits y el de amlo)\n",
    "para las frases: \"sino gano me voy a la chingada\", \"ya se va a acabar la corrupción\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Permutaciones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucción</kbd>_\n",
    "\n",
    "Para cada oración del punto anterior, haga todas las permutaciones posibles.\n",
    "Calcule su probabilidad a cada nueva frase y muestre el top 3 mas probable y el top 3\n",
    "menos probable (para ambos modelos de lenguaje). Proponga una frase más y haga lo\n",
    "mismo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
