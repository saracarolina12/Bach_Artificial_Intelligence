{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 4\n",
    "0226594 || Sara Carolina G√≥mez Delgado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from itertools import islice\n",
    "import random\n",
    "import mpmath\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_from_file(path_corpus, path_truth):\n",
    "    tr_txt = []\n",
    "    tr_y = []\n",
    "    with open(path_corpus, \"r\",encoding=\"utf8\") as f_corpus, open(path_truth, \"r\",encoding=\"utf8\") as f_truth:\n",
    "        for tuit in f_corpus:\n",
    "            tr_txt += [tuit]\n",
    "        for label in f_truth:\n",
    "            tr_y += [label] \n",
    "    return tr_txt, tr_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Calculate Perplexity_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(prob, N): \n",
    "    inv = 1/prob\n",
    "    return mpmath.root(inv, N)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h1 style=\"text-align: center;\">Parte 1</h1>\n",
    "\n",
    "<h4 style=\"text-align: center;\"><i>\"Modelos de Lenguaje y Evaluaci√≥n\"</i></h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Preprocesamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucci√≥n</kbd>_\n",
    "\n",
    "Preprocese todos los tuits de agresividad (positivos y negativos) seg√∫n su intuici√≥n para construir un buen corpus para un modelo de lenguaje (e.g., solo palabras en min√∫scula, etc.). Agregue tokens especiales de < s > y </ s > seg√∫n usted considere (e.g., al\n",
    "inicio y final de cada tuit). Defina su vocabulario y enmascare con < unk > toda palabra\n",
    "que no est√© en su vocabulario.\n",
    "\n",
    "\n",
    "_<kbd>Comentario</kbd>_\n",
    "\n",
    "En esta secci√≥n se preprocesaron los tweets y se gener√≥ un corpus donde s√≥lo se consideraron palabras (no emojis, no signos de puntuaci√≥n) y se eligi√≥ un vocabulario tomando las palabras √∫nicas en el training set, despu√©s se ordenaron seg√∫n su frecuencia (mayor a menor) y se tomaron s√≥lo las palabras cuya frecuencia fue mayor a 50 para formar el vocabulario final. Me parece importante resaltar, como principal conclusi√≥n, la importancia de descartar palabras que aparecen poco, ya que pueden llegar a afectar fuertemente al intentar aplicar un modelo de lenguaje."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Leer tweets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_txt, tr_y = get_texts_from_file(\"./mex_train.txt\", \"./mex_train_labels.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Pre-procesar tweets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TweetTokenizer instance\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <s> at the beggining of a tweet and </s> at the end of this tweet\n",
    "corpus_palabras = []\n",
    "for doc in tr_txt:\n",
    "    x = \"<s>\" + doc + \"</s>\"\n",
    "    corpus_palabras += tokenizer.tokenize(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'lo', 'peor', 'de', 'todo', 'es', 'que', 'no', 'me', 'dan']\n"
     ]
    }
   ],
   "source": [
    "processed = []\n",
    "for word in corpus_palabras:\n",
    "    if(np.char.isalpha(word) or word == \"<s>\" or word == \"</s>\"):\n",
    "            processed.append(word)\n",
    "print(processed[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(processed) # frecuencia de cada palabra\n",
    "def sortFreqDict(freqdict):\n",
    "    aux = [(freqdict[key], key) for key in freqdict] #lista de pares ordenada (m√°s frecuente a menos)\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    return aux #regresa el objeto ordenado en reversa (m√°s frecuentes a menos frecuentes)\n",
    "\n",
    "V = sortFreqDict(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', 'que', 'de', 'a', 'la', 'y', 'no', 'me', 'el']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if freq < 50, ignore it (don't let it be part of vocab)\n",
    "vocab = []\n",
    "for item in V:\n",
    "    if item[0] > 1:\n",
    "        vocab.append(item[1])\n",
    "vocab[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'lo', 'peor', 'de', 'todo', 'es', 'que', 'no', 'me', 'dan']\n"
     ]
    }
   ],
   "source": [
    "for i,word in enumerate(processed):\n",
    "    if word not in vocab:\n",
    "        processed[i] = \"<unk>\"\n",
    "print(processed[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Entrenamiento (unigrama, bigrama y trigrama)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucci√≥n</kbd>_\n",
    "\n",
    "Entrene tres modelos de lenguaje sobre todos los tuis: $P_{unigramas}(w^n_1), P_{bigramas}(w^n_1), P{trigramas}(w^n_1).$ Para cada uno proporcione  una interfaz (funci√≥n) sencilla para $P{n-grama}(w^n_1)$ y $Pn-grama(w^n_1 | w^{n-1}_{n-N+1})$. Los modelos modelos deben tener una estrategia com√∫n para lidiar con\n",
    "secuencias no vistas. Puede optar por un suavizamiento Laplace o un Good-Turing\n",
    "discounting. Muestre un par de ejemplos de como funciona, al menos uno con una\n",
    "palabra fuera del vocabulario.\n",
    "\n",
    "\n",
    "_<kbd>Comentario</kbd>_\n",
    "\n",
    "En esta secci√≥n decid√≠ crear dos funciones aplicando en ambas Laplace Smoothing (Add-One) para manejar el caso de secuencias no vistas.\n",
    "1) Funci√≥n que devuelve la probabilidad de que suceda una palabra dada un contexto.\n",
    "2) Funci√≥n que devuelve la probabilidad de que suceda una oraci√≥n.\n",
    "\n",
    "Como conclusion, me llam√≥ mucho la atenci√≥n el hecho de que cuando se encuentra el modelo con una palabra que no ha visto antes, no la vuelve cero (por al add-one de Laplace), si no que su probabilidad se vuelve muy peque√±a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0.0133005624\n",
      "bigram\n",
      "\t0.0013767184\n",
      "trigram\n",
      "\t0.0000404285\n",
      "bigram\n",
      "\t0.0000101145\n"
     ]
    }
   ],
   "source": [
    "def prob_word(words, context, vocab_size): # (list)\n",
    "    assert isinstance(words, list)\n",
    "    assert isinstance(context, list)\n",
    "    assert len(context) > 0\n",
    "    if len(words) == 1: # prob = (word_freq + k) / (context_size + k*V)\n",
    "        # print(\"unigram\")\n",
    "        word = words[0]\n",
    "        word_freq = context.count(word)\n",
    "        context_size = len(context)\n",
    "        prob = (word_freq+1)/(context_size+vocab_size)\n",
    "        return prob\n",
    "    elif len(words) == 2: # prob = (bigram_freq + k) / (a_freq + k*V)\n",
    "        print(\"bigram\")\n",
    "        a,b = words\n",
    "        a_freq = context.count(a)\n",
    "        bigram_freq = 0\n",
    "        for i in range(len(context)-1):\n",
    "            if context[i]==a and context[i+1]==b:\n",
    "                bigram_freq += 1\n",
    "        prob = (bigram_freq+1)/(a_freq+vocab_size)\n",
    "        return prob\n",
    "    elif len(words) == 3: # prob = (count_trigram + k) / (count_bigram + k*V)\n",
    "        print(\"trigram\")\n",
    "        count_trigram = 0\n",
    "        count_bigram = 0\n",
    "        for i in range(len(context)-2):\n",
    "            # number of times the trigram appears\n",
    "            if context[i:i+3] == words:\n",
    "                count_trigram += 1\n",
    "            # number of times the first two words of the trigram appear in the context (bigram)\n",
    "            if context[i:i+2] == words[:2]:\n",
    "                count_bigram += 1\n",
    "        prob = (count_trigram+1)/(count_bigram + vocab_size)\n",
    "        return prob\n",
    "\n",
    "# unigrama\n",
    "\n",
    "result = prob_word([\"a\"], processed, len(processed))\n",
    "print(\"\\t{:.10f}\".format(result))\n",
    "\n",
    "# bigrama\n",
    "result = prob_word([\"lo\", \"que\"], processed, len(processed))\n",
    "print(\"\\t{:.10f}\".format(result))\n",
    "\n",
    "# trigrama\n",
    "result = prob_word([\"si\", \"no\", \"fuera\"], processed, len(processed))\n",
    "print(\"\\t{:.10f}\".format(result))\n",
    "\n",
    "# These words don't belong to the vocabulary\n",
    "result = prob_word([\"hello\", \"dude\"], processed, len(processed))\n",
    "print(\"\\t{:.10f}\".format(result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram\n",
      "\t0.0313333086\n"
     ]
    }
   ],
   "source": [
    "def ngram_prob(n, sentence, corpus):\n",
    "    assert isinstance(n, int)\n",
    "    assert isinstance(sentence, list)\n",
    "    assert n <= len(sentence)\n",
    "    assert isinstance(corpus, list)\n",
    "\n",
    "    freq = {}\n",
    "    for i in range(len(corpus)-n+1):\n",
    "        gram = tuple(corpus[i:i+n]) # n-gram (1-3)\n",
    "        if gram in freq:\n",
    "            freq[gram] += 1\n",
    "        else:\n",
    "            freq[gram] = 1\n",
    "    # Laplace smoothing\n",
    "    V = len(set(corpus))\n",
    "    for gram in freq:\n",
    "        freq[gram] += 1\n",
    "    count = 0\n",
    "    for val in freq.values():\n",
    "        count += val\n",
    "    count += V\n",
    "    prob = (freq.get(tuple(sentence[-n:]),0)+1)/count\n",
    "    return prob\n",
    "\n",
    "ngram = 1\n",
    "result = ngram_prob(ngram, ['de', 'todo', 'lo', 'que'], processed)\n",
    "if ngram == 1: print(\"unigram\")\n",
    "elif ngram == 2: print(\"bigram\")\n",
    "elif ngram == 3: print(\"trigram\")\n",
    "else: print(\"n-gram\")\n",
    "print(\"\\t{:.10f}\".format(result))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Construcci√≥n de Modelo Interpolado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucci√≥n</kbd>_\n",
    "\n",
    "Construya un modelo interpolado con valores $\\lambda$ fijos:\n",
    "\n",
    "$$\\hat{P}(w_n|w_{n-2}w_{n-1}) = \\lambda_1P(w_n|w_{n-2}w_{n-1}) + \\lambda_2P(w_n|w_{n-1}) + \\lambda_3P(w_n)$$\n",
    "\n",
    "Para ello experimente con el modelo en particiones estratificadas de 80%, 10% y 10% para\n",
    "entrenar (train), ajuste de par√°metros (val) y prueba (test) respectivamente. Muestre como\n",
    "bajan o suben las perplejidades en validaci√≥n, finalmente pruebe una vez en test. Para esto puede explorar algunos valores ‚ÉóŒª y elija el mejor, i.e., [1/3, 1/3, 1/3],[.4, .4, .2],[.2, .4, .4],[.5, .4, .1]\n",
    "y [.1, .4, .5].\n",
    "\n",
    "_<kbd>Comentario</kbd>_\n",
    "\n",
    "En esta secci√≥n divid√≠ primero mis datos en 80% train, 10% validation y 10% test. Despu√©s, cre√© el modelo interpolado donde $\\lambda_1P(w_n|w_{n-2}w_{n-1})$ representa un trigrama multiplicado por un valor lambda_1, \\lambda_2P(w_n|w_{n-1}) representa un bigrama multiplicado por un valor de lambda_2 y \\lambda_3P(w_n) representa un unigrama multiplicado por un valor de lambda_3. Los resultados son sumados. Sum√© estos valores y finalmente experiment√© con los diferentes valores de lambda para comparar sus perplejidades y quedarme con el valor m√°s peque√±o."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Data Partition (80% (train), 10% validation, 10% test)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  79.99959542015617\n",
      "Validation:  10.000202289921916\n",
      "Test:  10.000202289921916\n"
     ]
    }
   ],
   "source": [
    "# Shuffle data\n",
    "rand_data = processed.copy()\n",
    "random.shuffle(rand_data)\n",
    "\n",
    "\n",
    "# Divide 80% Train, 20% Test\n",
    "n = len(rand_data) \n",
    "n_train = int(n * 0.8)  # entrenamiento (80%)\n",
    "n_test = n - n_train  # prueba (20%)\n",
    "train = rand_data[:n_train]\n",
    "to_divide = rand_data[n_train:]\n",
    "\n",
    "# Divide that 20% in two parts (10% Validation, 10% Test)\n",
    "n_test = len(to_divide) \n",
    "n_val = n_test//2  # Validation (50%)\n",
    "n_test = n_test - n_val  # Test(50%)\n",
    "validation = to_divide[:n_val]\n",
    "test = to_divide[n_val:]\n",
    "\n",
    "print(\"Train: \",(len(train)*100)/len(processed))\n",
    "print(\"Validation: \",(len(validation)*100)/len(processed))\n",
    "print(\"Test: \",(len(test)*100)/len(processed))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Find Best Lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_lambda(uni, bi, tri, lambdas, N):\n",
    "    assert len(lambdas) >= 1\n",
    "    best_lam = lambdas[0]\n",
    "    best_perplexity = 10000\n",
    "    for l in lambdas:\n",
    "        probability = uni*l[0] + bi*l[1] + tri*l[2]\n",
    "        p = perplexity(probability, N)\n",
    "        if(p < best_perplexity):\n",
    "            best_perplexity = p\n",
    "            best_lam = [l[0], l[1], l[2]]\n",
    "    return best_lam, best_perplexity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Validation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram: 0.0119531410\n",
      "Bigram: 0.0002549293\n",
      "Trigram: 0.0000064259\n",
      "best lambda and its perplexity:  [0.5, 0.4, 0.1] 1.0005162538436\n"
     ]
    }
   ],
   "source": [
    "lambdas = [[1/3, 1/3, 1/3], [.4, .4, .2], [.2, .4, .4], [.5, .4, .1], [.1, .4, .5]]\n",
    "\n",
    "# unigram\n",
    "ngram = 1\n",
    "uni = ngram_prob(ngram, validation, train)\n",
    "print(\"Unigram: {:.10f}\".format(uni))\n",
    "\n",
    "# bigram\n",
    "ngram = 2\n",
    "bi = ngram_prob(ngram, validation, train)\n",
    "print(\"Bigram: {:.10f}\".format(bi))\n",
    "\n",
    "# trigram\n",
    "ngram = 3\n",
    "tri = ngram_prob(ngram, validation, train)\n",
    "print(\"Trigram: {:.10f}\".format(tri))\n",
    "\n",
    "res = best_lambda(uni,bi,tri, lambdas, len(validation))\n",
    "validation_best_lambda = res[0]\n",
    "validation_best_perplexity = res[1]\n",
    "print(\"best lambda and its perplexity: \",validation_best_lambda, validation_best_perplexity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram: 0.0004654119\n",
      "Bigram: 0.0000398327\n",
      "Trigram: 0.0000064259\n",
      "best lambda and its perplexity:  [0.5, 0.4, 0.1] 1.00083952760948\n"
     ]
    }
   ],
   "source": [
    "lambdas = [[1/3, 1/3, 1/3], [.4, .4, .2], [.2, .4, .4], [.5, .4, .1], [.1, .4, .5]]\n",
    "\n",
    "# unigram\n",
    "ngram = 1\n",
    "uni = ngram_prob(ngram, test , train)\n",
    "print(\"Unigram: {:.10f}\".format(uni))\n",
    "\n",
    "# bigram\n",
    "ngram = 2\n",
    "bi = ngram_prob(ngram, test , train)\n",
    "print(\"Bigram: {:.10f}\".format(bi))\n",
    "\n",
    "# trigram\n",
    "ngram = 3\n",
    "tri = ngram_prob(ngram, test , train)\n",
    "print(\"Trigram: {:.10f}\".format(tri))\n",
    "\n",
    "res = best_lambda(uni,bi,tri, lambdas, len(test))\n",
    "test_best_lambda = res[0]\n",
    "test_best_perplexity = res[1]\n",
    "print(\"best lambda and its perplexity: \", test_best_lambda, test_best_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Show Best Perplexity_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Perplexity (validation):  1.0005162538436\n",
      "Lambdas:  [0.5, 0.4, 0.1]\n"
     ]
    }
   ],
   "source": [
    "if validation_best_perplexity < test_best_perplexity:\n",
    "    print(\"Best Perplexity (validation): \", validation_best_perplexity)\n",
    "    print(\"Lambdas: \", validation_best_lambda)\n",
    "else:\n",
    "    print(\"Best Perplexity (test): \", test_best_perplexity)\n",
    "    print(\"Lambdas: \", test_best_lambda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h1 style=\"text-align: center;\">Parte 2</h1>\n",
    "\n",
    "<h4 style=\"text-align: center;\"><i>\"Generaci√≥n de Texto\"</i></h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta parte reentrenar√° su modelo de lenguaje interpolado para aprender los valores Œª:\n",
    "\n",
    "$$\\hat{P}(w_n|w_{n-2}w_{n-1}) = \\lambda_1P(w_n|w_{n-2}w_{n-1}) + \\lambda_2P(w_n|w_{n-1}) + \\lambda_3P(w_n)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Funci√≥n \"tuitear\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucci√≥n</kbd>_\n",
    "\n",
    "Haga una funci√≥n \"tuitear\" con base en su modelo de lenguaje PÃÇ del √∫ltimo punto.\n",
    "El modelo deber√° poder parar autom√°ticamente cuando genere el s√≠mbolo de terminaci√≥n\n",
    "de tuit al final (e.g., \"< /s >\"), o 50 palabras. Proponga algo para que en los √∫ltimos tokens\n",
    "sea m√°s probable generar el token \"</ s >\". Muestre al menos cinco ejemplos.\n",
    "\n",
    "\n",
    "_<kbd>Comentario</kbd>_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplos:\n",
      "\n",
      "#1 ('hijos tienen pase </s> ', 4)\n",
      "\n",
      "#2 ('que t√∫ tengo el hablan me üò° </s> ', 8)\n",
      "\n",
      "#3 ('caga oh ver a <s> en putos 2 </s> ', 9)\n",
      "\n",
      "#4 ('y del siente que ... al </s> ', 7)\n",
      "\n",
      "#5 ('siempre s mi </s> ', 4)\n"
     ]
    }
   ],
   "source": [
    "def unigrams(corpus):\n",
    "    freq = defaultdict(int)\n",
    "    total_words = len(corpus)\n",
    "    unigrams = {}\n",
    "    \n",
    "    for word in corpus:\n",
    "        freq[word] += 1\n",
    "    \n",
    "    for word, count in freq.items():\n",
    "        prob = count/total_words\n",
    "        unigrams[word] = prob\n",
    "    return unigrams\n",
    "    \n",
    "\n",
    "def tuitear(max_words, model):\n",
    "    # model[\"<s>\"] = model.get(\"<s>\", 0.0) + 0.1\n",
    "    model[\"</s>\"] = model.get(\"</s>\", 0.0) + 0.1\n",
    "    s = \"\"\n",
    "    for i in range(max_words):\n",
    "        r = random.random()\n",
    "        cumulative_prob = 0\n",
    "        for word, prob in model.items():\n",
    "            cumulative_prob += prob\n",
    "            if r < cumulative_prob:\n",
    "                s += word + \" \"\n",
    "                break\n",
    "        if s.endswith(\"</s> \"):\n",
    "            break\n",
    "    tokenized_sentence =  tokenizer.tokenize(s)\n",
    "    return s, len(tokenized_sentence)\n",
    "\n",
    "print(\"Ejemplos:\")\n",
    "for i in range(5):\n",
    "    print(f\"\\n#{i+1}\",tuitear(50, unigrams(corpus_palabras)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Entrenar modelo de lenguaje AMLO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucci√≥n</kbd>_\n",
    "\n",
    "Use la intuici√≥n que ha ganado en esta tarea y los datos de las ma√±aneras para\n",
    "entrenar un modelo de lenguaje AMLO. Haga una un funci√≥n \"dar_conferencia()\". Gener√©\n",
    "un discurso de 300 palabras y detenga al modelo de forma abrupta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dar_conferencia():\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Experimentando con dos frases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucci√≥n</kbd>_\n",
    "\n",
    "Calcule el estimado de cada uno sus modelos de lenguaje (el de tuits y el de amlo)\n",
    "para las frases: \"sino gano me voy a la chingada\", \"ya se va a acabar la corrupci√≥n\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Permutaciones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_<kbd>Instrucci√≥n</kbd>_\n",
    "\n",
    "Para cada oraci√≥n del punto anterior, haga todas las permutaciones posibles.\n",
    "Calcule su probabilidad a cada nueva frase y muestre el top 3 mas probable y el top 3\n",
    "menos probable (para ambos modelos de lenguaje). Proponga una frase m√°s y haga lo\n",
    "mismo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
