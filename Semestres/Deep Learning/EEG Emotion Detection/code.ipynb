{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import torch\n",
    "import pickle\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import import_ipynb\n",
    "from utils import load_eeg_data\n",
    "from utils import load_eye_data\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_movement_features = './dataset/Eye_movement_features'\n",
    "eeg_features = './dataset/EEG_DE_features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** 1_123.npz ********************\n",
      "EEG DATA\n",
      "[[11.08252158  8.91598993  7.89408803 ...  7.07195532  5.65708081\n",
      "   4.36150606]\n",
      " [11.08181639  8.91510401  7.89364634 ...  7.07177633  5.6569868\n",
      "   4.36162375]\n",
      " [11.08113882  8.91424457  7.89311581 ...  7.07146188  5.65674597\n",
      "   4.36177956]\n",
      " ...\n",
      " [11.09093681  8.90651219  7.88062322 ...  7.05494398  5.64746577\n",
      "   4.36830008]\n",
      " [11.09181965  8.90597236  7.87993625 ...  7.05439539  5.64685332\n",
      "   4.36912144]\n",
      " [11.09237631  8.90556688  7.87957235 ...  7.05379577  5.64648296\n",
      "   4.36975357]]\n",
      "\tSession 1 - Clip #1 ----> Happy\n",
      "\tSession 1 - Clip #2 ----> Fear\n",
      "\tSession 1 - Clip #3 ----> Neutral\n",
      "\tSession 1 - Clip #4 ----> Sad\n",
      "\tSession 1 - Clip #5 ----> Disgust\n",
      "\tSession 1 - Clip #6 ----> Happy\n",
      "\tSession 1 - Clip #7 ----> Fear\n",
      "\tSession 1 - Clip #8 ----> Neutral\n",
      "\tSession 1 - Clip #9 ----> Sad\n",
      "\tSession 1 - Clip #10 ----> Disgust\n",
      "\tSession 1 - Clip #11 ----> Happy\n",
      "\tSession 1 - Clip #12 ----> Fear\n",
      "\tSession 1 - Clip #13 ----> Neutral\n",
      "\tSession 1 - Clip #14 ----> Sad\n",
      "\tSession 1 - Clip #15 ----> Disgust\n",
      "\tSession 2 - Clip #1 ----> Sad\n",
      "\tSession 2 - Clip #2 ----> Fear\n",
      "\tSession 2 - Clip #3 ----> Neutral\n",
      "\tSession 2 - Clip #4 ----> Disgust\n",
      "\tSession 2 - Clip #5 ----> Happy\n",
      "\tSession 2 - Clip #6 ----> Happy\n",
      "\tSession 2 - Clip #7 ----> Disgust\n",
      "\tSession 2 - Clip #8 ----> Neutral\n",
      "\tSession 2 - Clip #9 ----> Sad\n",
      "\tSession 2 - Clip #10 ----> Fear\n",
      "\tSession 2 - Clip #11 ----> Neutral\n",
      "\tSession 2 - Clip #12 ----> Happy\n",
      "\tSession 2 - Clip #13 ----> Fear\n",
      "\tSession 2 - Clip #14 ----> Sad\n",
      "\tSession 2 - Clip #15 ----> Disgust\n",
      "\tSession 3 - Clip #1 ----> Sad\n",
      "\tSession 3 - Clip #2 ----> Fear\n",
      "\tSession 3 - Clip #3 ----> Neutral\n",
      "\tSession 3 - Clip #4 ----> Disgust\n",
      "\tSession 3 - Clip #5 ----> Happy\n",
      "\tSession 3 - Clip #6 ----> Happy\n",
      "\tSession 3 - Clip #7 ----> Disgust\n",
      "\tSession 3 - Clip #8 ----> Neutral\n",
      "\tSession 3 - Clip #9 ----> Sad\n",
      "\tSession 3 - Clip #10 ----> Fear\n",
      "\tSession 3 - Clip #11 ----> Neutral\n",
      "\tSession 3 - Clip #12 ----> Happy\n",
      "\tSession 3 - Clip #13 ----> Fear\n",
      "\tSession 3 - Clip #14 ----> Sad\n",
      "\tSession 3 - Clip #15 ----> Disgust\n",
      "EYE TRACKING DATA\n",
      "\tSession 1 - Clip #1 ----> Happy\n",
      "\tSession 1 - Clip #2 ----> Fear\n",
      "\tSession 1 - Clip #3 ----> Neutral\n",
      "\tSession 1 - Clip #4 ----> Sad\n",
      "\tSession 1 - Clip #5 ----> Disgust\n",
      "\tSession 1 - Clip #6 ----> Happy\n",
      "\tSession 1 - Clip #7 ----> Fear\n",
      "\tSession 1 - Clip #8 ----> Neutral\n",
      "\tSession 1 - Clip #9 ----> Sad\n",
      "\tSession 1 - Clip #10 ----> Disgust\n",
      "\tSession 1 - Clip #11 ----> Happy\n",
      "\tSession 1 - Clip #12 ----> Fear\n",
      "\tSession 1 - Clip #13 ----> Neutral\n",
      "\tSession 1 - Clip #14 ----> Sad\n",
      "\tSession 1 - Clip #15 ----> Disgust\n",
      "\tSession 2 - Clip #1 ----> Sad\n",
      "\tSession 2 - Clip #2 ----> Fear\n",
      "\tSession 2 - Clip #3 ----> Neutral\n",
      "\tSession 2 - Clip #4 ----> Disgust\n",
      "\tSession 2 - Clip #5 ----> Happy\n",
      "\tSession 2 - Clip #6 ----> Happy\n",
      "\tSession 2 - Clip #7 ----> Disgust\n",
      "\tSession 2 - Clip #8 ----> Neutral\n",
      "\tSession 2 - Clip #9 ----> Sad\n",
      "\tSession 2 - Clip #10 ----> Fear\n",
      "\tSession 2 - Clip #11 ----> Neutral\n",
      "\tSession 2 - Clip #12 ----> Happy\n",
      "\tSession 2 - Clip #13 ----> Fear\n",
      "\tSession 2 - Clip #14 ----> Sad\n",
      "\tSession 2 - Clip #15 ----> Disgust\n",
      "\tSession 3 - Clip #1 ----> Sad\n",
      "\tSession 3 - Clip #2 ----> Fear\n",
      "\tSession 3 - Clip #3 ----> Neutral\n",
      "\tSession 3 - Clip #4 ----> Disgust\n",
      "\tSession 3 - Clip #5 ----> Happy\n",
      "\tSession 3 - Clip #6 ----> Happy\n",
      "\tSession 3 - Clip #7 ----> Disgust\n",
      "\tSession 3 - Clip #8 ----> Neutral\n",
      "\tSession 3 - Clip #9 ----> Sad\n",
      "\tSession 3 - Clip #10 ----> Fear\n",
      "\tSession 3 - Clip #11 ----> Neutral\n",
      "\tSession 3 - Clip #12 ----> Happy\n",
      "\tSession 3 - Clip #13 ----> Fear\n",
      "\tSession 3 - Clip #14 ----> Sad\n",
      "\tSession 3 - Clip #15 ----> Disgust\n"
     ]
    }
   ],
   "source": [
    "eeg_dir = eeg_features\n",
    "eye_dir = eye_movement_features\n",
    "file_list = os.listdir(eeg_dir)\n",
    "file_list.sort()\n",
    "\n",
    "res_dir = './res/cv3/'\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)\n",
    "cv = 1\n",
    "\n",
    "for f_id in file_list:\n",
    "    print(f\"******************** {f_id} ********************\")\n",
    "    print(\"EEG DATA\")\n",
    "    load_eeg_data(eeg_dir, f_id, True)\n",
    "    print(\"EYE TRACKING DATA\")\n",
    "    load_eye_data(eye_dir, f_id, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_raw = './dataset/EEG_raw'\n",
    "eye_raw = './dataset/Eye_raw'\n",
    "\n",
    "eeg_dir = eeg_raw\n",
    "eye_dir = eye_raw\n",
    "file_list = os.listdir(eeg_dir)\n",
    "file_list.sort()\n",
    "\n",
    "res_dir = './res/cv3/'\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_1_20180804.cnt\n",
      "<RawCNT | 1_1_20180804.cnt, 66 x 3378360 (3378.4 s), ~81 kB, data not loaded>\n",
      "\n",
      "\n",
      "<Info | 9 non-empty values\n",
      " bads: []\n",
      " ch_names: FP1, FPZ, FP2, AF3, AF4, F7, F5, F3, F1, FZ, F2, F4, F6, F8, ...\n",
      " chs: 66 EEG\n",
      " custom_ref_applied: False\n",
      " dig: 69 items (3 Cardinal, 66 EEG)\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 500.0 Hz\n",
      " meas_date: 2018-04-08 17:35:05 UTC\n",
      " nchan: 66\n",
      " projs: []\n",
      " sfreq: 1000.0 Hz\n",
      " subject_info: 5 items (dict)\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "# EEG RAW DATA\n",
    "for f_id in file_list:\n",
    "    print(f_id)\n",
    "    eeg_raw = mne.io.read_raw_cnt(f'./dataset/EEG_raw/{f_id}')\n",
    "    print(eeg_raw)\n",
    "    print('\\n')\n",
    "    print(eeg_raw.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FP1', 'FPZ', 'FP2', 'AF3', 'AF4', 'F7', 'F5', 'F3', 'F1', 'FZ', 'F2', 'F4', 'F6', 'F8', 'FT7', 'FC5', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', 'FC6', 'FT8', 'T7', 'C5', 'C3', 'C1', 'CZ', 'C2', 'C4', 'C6', 'T8', 'TP7', 'CP5', 'CP3', 'CP1', 'CPZ', 'CP2', 'CP4', 'CP6', 'TP8', 'P7', 'P5', 'P3', 'P1', 'PZ', 'P2', 'P4', 'P6', 'P8', 'PO7', 'PO5', 'PO3', 'POZ', 'PO4', 'PO6', 'PO8', 'CB1', 'O1', 'OZ', 'O2', 'CB2']\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "ch_names = eeg_raw.ch_names\n",
    "\n",
    "# drop non-used channels\n",
    "useless_ch = ['M1', 'M2', 'VEO', 'HEO']\n",
    "eeg_raw.drop_channels(useless_ch)\n",
    "new_ch = eeg_raw.ch_names\n",
    "print(new_ch)\n",
    "print(len(new_ch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data with MNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = eeg_raw.plot()\n",
    "# plt.savefig(\"./images/EEG_raw_data.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.93824482e-05 -4.80115414e-05 -4.97996807e-05 ...  1.78605318e-04\n",
      "   1.67906284e-04  1.55389309e-04]\n",
      " [-4.86969948e-05 -5.03957272e-05 -5.25712967e-05 ...  1.54405832e-04\n",
      "   1.47789717e-04  1.38103962e-04]\n",
      " [-6.88433647e-05 -7.08997250e-05 -7.05718994e-05 ...  1.60545111e-04\n",
      "   1.58250332e-04  1.51783228e-04]\n",
      " ...\n",
      " [-2.04741955e-05 -2.02655792e-05 -2.08616257e-05 ... -1.18255615e-04\n",
      "  -1.06900930e-04 -8.60989094e-05]\n",
      " [-2.67922878e-05 -2.62856483e-05 -2.57194042e-05 ... -5.71608543e-05\n",
      "  -5.07831573e-05 -3.39746475e-05]\n",
      " [ 2.27987766e-05  2.33948231e-05  2.43484974e-05 ... -3.70323658e-04\n",
      "  -3.55124474e-04 -3.30239534e-04]]\n"
     ]
    }
   ],
   "source": [
    "data_matrix = eeg_raw.get_data()\n",
    "print(data_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamp EEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session #3\n",
    "start_second = [30, 353, 478, 674, 825, 908, 1200, 1346, 1451, 1711, 2055, 2307, 2457, 2726, 2888]\n",
    "end_second = [321, 418, 643, 764, 877, 1147, 1284, 1418, 1679, 1996, 2275, 2425, 2664, 2857, 3066]\n",
    "sample_freq = 1000\n",
    "\n",
    "data_trial_1 = data_matrix[:, start_second[0]*1000 : end_second[0]*1000]\n",
    "data_trial_5 = data_matrix[:, start_second[4]*1000 : end_second[4]*1000]\n",
    "data_trial_15 = data_matrix[:, start_second[14]*1000 : end_second[14]*1000]\n",
    "\n",
    "# print(data_trial_1)\n",
    "# print(data_trial_5)\n",
    "# print(data_trial_15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_raw = './dataset/EEG_raw/1_1_20180804.cnt'\n",
    "eye_raw = './dataset/Eye_raw/Session_1/10_1_20180507.xlsx'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "    def __init__(self, eeg_raw, eye_raw):\n",
    "        self.eeg_data = mne.io.read_raw_cnt(eeg_raw)\n",
    "        self.eye_data = pd.read_excel(eye_raw)\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.eeg_data)\n",
    "        return min(len(self.eeg_data), len(self.eye_data))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        eeg_channel_name = 'FP1'  \n",
    "        eeg_signal = self.eeg_data[eeg_channel_name][0][0]\n",
    "\n",
    "        eye_signal = self.eye_data.iloc[index, 0]\n",
    "\n",
    "        eeg_signal_tensor = torch.tensor(eeg_signal)\n",
    "        eye_signal_tensor = torch.tensor(eye_signal)\n",
    "\n",
    "        eeg_mean = torch.mean(eeg_signal_tensor)\n",
    "        eeg_std = torch.std(eeg_signal_tensor)\n",
    "        eye_mean = torch.mean(eye_signal_tensor)\n",
    "        eye_std = torch.std(eye_signal_tensor)\n",
    "\n",
    "        # Normalize the signal data\n",
    "        eeg_signal_tensor = (eeg_signal_tensor - eeg_mean) / eeg_std\n",
    "        eye_signal_tensor = (eye_signal_tensor - eye_mean) / eye_std\n",
    "\n",
    "        return eeg_signal_tensor, eeg_mean, eeg_std, eye_signal_tensor, eye_mean, eye_std\n",
    "\n",
    "# Create a dataset from the EEG and eye files\n",
    "dataset = myDataset(eeg_raw, eye_raw)\n",
    "# Create a data loader for the dataset\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.shape = 151 samples\n",
    "# tamaño de cada sample:  6 (eeg_signal_tensor, eeg_mean, eeg_std, eye_signal_tensor, eye_mean, eye_std)\n",
    "\n",
    "# print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over data_loader\n",
    "\n",
    "# next(iter(data_loader))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot EEG_signal_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scago\\AppData\\Local\\Temp\\ipykernel_15960\\1645269431.py:17: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "channel = 'FP1'\n",
    "\n",
    "# Assuming your tensor is named 'eeg_tensor' and has shape (3378360,)\n",
    "eeg_channel_data = dataset[0][0]\n",
    "\n",
    "# Create a time vector corresponding to the data\n",
    "time = range(len(eeg_channel_data))\n",
    "time = [t/1000 for t in time]  # Convert to seconds\n",
    "\n",
    "# Create a line plot of the EEG channel data\n",
    "plt.plot(time, eeg_channel_data)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude (uV)')\n",
    "plt.title('EEG data of channel FP1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n",
    "        # Initialize cell state with zeros\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n",
    "        # 1st time-step\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        # output layer\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/600], Loss: 0.4869, Accuracy: 85.00%\n",
      "Epoch [1/10], Step [200/600], Loss: 0.1325, Accuracy: 96.00%\n",
      "Epoch [1/10], Step [300/600], Loss: 0.2054, Accuracy: 92.00%\n",
      "Epoch [1/10], Step [400/600], Loss: 0.2046, Accuracy: 95.00%\n",
      "Epoch [1/10], Step [500/600], Loss: 0.1024, Accuracy: 96.00%\n",
      "Epoch [1/10], Step [600/600], Loss: 0.1607, Accuracy: 93.00%\n",
      "Epoch [2/10], Step [100/600], Loss: 0.0852, Accuracy: 98.00%\n",
      "Epoch [2/10], Step [200/600], Loss: 0.1344, Accuracy: 95.00%\n",
      "Epoch [2/10], Step [300/600], Loss: 0.0259, Accuracy: 100.00%\n",
      "Epoch [2/10], Step [400/600], Loss: 0.1060, Accuracy: 95.00%\n",
      "Epoch [2/10], Step [500/600], Loss: 0.0570, Accuracy: 99.00%\n",
      "Epoch [2/10], Step [600/600], Loss: 0.0920, Accuracy: 97.00%\n",
      "Epoch [3/10], Step [100/600], Loss: 0.0715, Accuracy: 98.00%\n",
      "Epoch [3/10], Step [200/600], Loss: 0.0128, Accuracy: 100.00%\n",
      "Epoch [3/10], Step [300/600], Loss: 0.0666, Accuracy: 96.00%\n",
      "Epoch [3/10], Step [400/600], Loss: 0.0230, Accuracy: 100.00%\n",
      "Epoch [3/10], Step [500/600], Loss: 0.0109, Accuracy: 100.00%\n",
      "Epoch [3/10], Step [600/600], Loss: 0.0864, Accuracy: 98.00%\n",
      "Epoch [4/10], Step [100/600], Loss: 0.0571, Accuracy: 98.00%\n",
      "Epoch [4/10], Step [200/600], Loss: 0.0083, Accuracy: 100.00%\n",
      "Epoch [4/10], Step [300/600], Loss: 0.0548, Accuracy: 99.00%\n",
      "Epoch [4/10], Step [400/600], Loss: 0.0252, Accuracy: 99.00%\n",
      "Epoch [4/10], Step [500/600], Loss: 0.0167, Accuracy: 100.00%\n",
      "Epoch [4/10], Step [600/600], Loss: 0.0368, Accuracy: 99.00%\n",
      "Epoch [5/10], Step [100/600], Loss: 0.0156, Accuracy: 100.00%\n",
      "Epoch [5/10], Step [200/600], Loss: 0.0155, Accuracy: 99.00%\n",
      "Epoch [5/10], Step [300/600], Loss: 0.1302, Accuracy: 98.00%\n",
      "Epoch [5/10], Step [400/600], Loss: 0.0145, Accuracy: 100.00%\n",
      "Epoch [5/10], Step [500/600], Loss: 0.0428, Accuracy: 99.00%\n",
      "Epoch [5/10], Step [600/600], Loss: 0.1296, Accuracy: 98.00%\n",
      "Epoch [6/10], Step [100/600], Loss: 0.0391, Accuracy: 99.00%\n",
      "Epoch [6/10], Step [200/600], Loss: 0.0201, Accuracy: 99.00%\n",
      "Epoch [6/10], Step [300/600], Loss: 0.1482, Accuracy: 96.00%\n",
      "Epoch [6/10], Step [400/600], Loss: 0.1002, Accuracy: 97.00%\n",
      "Epoch [6/10], Step [500/600], Loss: 0.1560, Accuracy: 97.00%\n",
      "Epoch [6/10], Step [600/600], Loss: 0.1092, Accuracy: 98.00%\n",
      "Epoch [7/10], Step [100/600], Loss: 0.0094, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [200/600], Loss: 0.0300, Accuracy: 98.00%\n",
      "Epoch [7/10], Step [300/600], Loss: 0.0131, Accuracy: 99.00%\n",
      "Epoch [7/10], Step [400/600], Loss: 0.0215, Accuracy: 98.00%\n",
      "Epoch [7/10], Step [500/600], Loss: 0.0347, Accuracy: 99.00%\n",
      "Epoch [7/10], Step [600/600], Loss: 0.1455, Accuracy: 97.00%\n",
      "Epoch [8/10], Step [100/600], Loss: 0.0144, Accuracy: 99.00%\n",
      "Epoch [8/10], Step [200/600], Loss: 0.1444, Accuracy: 97.00%\n",
      "Epoch [8/10], Step [300/600], Loss: 0.0853, Accuracy: 96.00%\n",
      "Epoch [8/10], Step [400/600], Loss: 0.0871, Accuracy: 95.00%\n",
      "Epoch [8/10], Step [500/600], Loss: 0.0319, Accuracy: 99.00%\n",
      "Epoch [8/10], Step [600/600], Loss: 0.0141, Accuracy: 99.00%\n",
      "Epoch [9/10], Step [100/600], Loss: 0.0963, Accuracy: 99.00%\n",
      "Epoch [9/10], Step [200/600], Loss: 0.1426, Accuracy: 97.00%\n",
      "Epoch [9/10], Step [300/600], Loss: 0.0131, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [400/600], Loss: 0.1157, Accuracy: 96.00%\n",
      "Epoch [9/10], Step [500/600], Loss: 0.0555, Accuracy: 98.00%\n",
      "Epoch [9/10], Step [600/600], Loss: 0.1143, Accuracy: 98.00%\n",
      "Epoch [10/10], Step [100/600], Loss: 0.0266, Accuracy: 98.00%\n",
      "Epoch [10/10], Step [200/600], Loss: 0.0490, Accuracy: 98.00%\n",
      "Epoch [10/10], Step [300/600], Loss: 0.0324, Accuracy: 99.00%\n",
      "Epoch [10/10], Step [400/600], Loss: 0.0551, Accuracy: 98.00%\n",
      "Epoch [10/10], Step [500/600], Loss: 0.0622, Accuracy: 99.00%\n",
      "Epoch [10/10], Step [600/600], Loss: 0.0211, Accuracy: 99.00%\n",
      "Test Accuracy: 98.29%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = 10\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create LSTM model\n",
    "lstm = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape input data\n",
    "        images = images.view(-1, input_size, input_size)\n",
    "        # Forward pass\n",
    "        outputs = lstm(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print loss and accuracy\n",
    "        if (i+1) % 100 == 0:\n",
    "            total = labels.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.item(), (correct/total)*100))\n",
    "\n",
    "# Evaluate the model\n",
    "lstm.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, input_size, input_size)\n",
    "        outputs = lstm(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Test Accuracy: {:.2f}%'.format((correct/total)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (873592335.py, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[37], line 25\u001b[1;36m\u001b[0m\n\u001b[1;33m    train_dataset =  # dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Pruebaaaaaa (sin usar SEED-V dataset)\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = 10\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "# Load MNIST dataset\n",
    "# train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "# test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "for f_id in file_list:\n",
    "    train_dataset =  # dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "    test_dataset = load_eeg_data(eeg_dir, f_id, False)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create LSTM model\n",
    "lstm = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape input data\n",
    "        images = images.view(-1, input_size, input_size)\n",
    "        # Forward pass\n",
    "        outputs = lstm(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print loss and accuracy\n",
    "        if (i+1) % 100 == 0:\n",
    "            total = labels.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.item(), (correct/total)*100))\n",
    "\n",
    "# Evaluate the model\n",
    "lstm.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, input_size, input_size)\n",
    "        outputs = lstm(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Test Accuracy: {:.2f}%'.format((correct/total)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36d8014425df087c299dae688a2df59daa7f4b005992d2496cc7a95dceb622c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
